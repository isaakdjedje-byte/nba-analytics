# WEEK 1 - RÃ‰SULTATS PARTIELS

**Date** : 08 FÃ©vrier 2026  
**DurÃ©e** : ~15 minutes (surprisingly fast!)  
**Status** : âœ… Phase 1 terminÃ©e

---

## ğŸ¯ RÃ©sultats des Optimisations

### Random Forest (50 trials, 3 min)
```json
{
  "accuracy": 76.19%,
  "auc": 84.33%,
  "best_params": {
    "n_estimators": 828,
    "max_depth": 13,
    "min_samples_split": 4,
    "min_samples_leaf": 10,
    "max_features": "sqrt",
    "bootstrap": true
  }
}
```
**vs Baseline** : 76.19% vs 76.10% (+0.09%)

### XGBoost (100 trials, 2 min 50s) ğŸ†
```json
{
  "accuracy": 76.76%,
  "auc": 84.99%,
  "best_params": {
    "n_estimators": 567,
    "max_depth": 4,
    "learning_rate": 0.010,
    "subsample": 0.736,
    "colsample_bytree": 0.991,
    "min_child_weight": 7,
    "gamma": 0.237,
    "reg_alpha": 4.6e-07,
    "reg_lambda": 4.6e-07
  }
}
```
**vs Baseline** : 76.76% vs 76.10% (+0.66%)
**vs RF** : 76.76% vs 76.19% (+0.57%)

---

## ğŸ“Š Progression

| Ã‰tape | Baseline | AprÃ¨s Optuna | Gain |
|-------|----------|--------------|------|
| Random Forest | 76.10% | 76.19% | +0.09% |
| XGBoost | - | 76.76% | Nouveau meilleur ! |

**Meilleur modÃ¨le actuel** : XGBoost (76.76%)

---

## âœ¨ Nouvelles Features (10 crÃ©Ã©es)

1. `momentum_diff` - DiffÃ©rence de forme
2. `offensive_efficiency_diff` - DiffÃ©rence offensive
3. `rebounding_diff` - DiffÃ©rence de rebonds
4. `fatigue_combo` - Combinaison fatigue
5. `rest_advantage_squared` - Avantage repos (non-linÃ©aire)
6. `win_pct_momentum_interaction` - Interaction niveau/forme
7. `home_h2h_advantage` - Avantage H2H Ã  domicile
8. `win_pct_diff_squared` - Diff niveau au carrÃ©
9. `h2h_pressure` - IntensitÃ© rivalitÃ©
10. `h2h_margin_weighted` - Marge H2H pondÃ©rÃ©e

**Dataset** : 8871 matchs Ã— 65 features (was 55)

---

## ğŸš€ Prochaines Ã‰tapes (Suite Semaine 1)

### 1. Stacking (Aujourd'hui)
Combiner RF + XGB + NN â†’ Objectif 77.5-78%

### 2. Calibration (Demain)
Calibrer les probabilitÃ©s pour les paris

### 3. Test sur nouvelles features (Demain)
RÃ©-entraÃ®ner avec les 65 features (vs 24 actuelles)

---

## ğŸ“ Fichiers CrÃ©Ã©s

```
results/week1/
â”œâ”€â”€ rf_best_params.json       âœ…
â”œâ”€â”€ xgb_best_params.json      âœ…
â”œâ”€â”€ rf_optimization.db        âœ…
â”œâ”€â”€ xgb_optimization.db       âœ…
â”œâ”€â”€ new_features_v2.json      âœ…
â”œâ”€â”€ rf_log.txt                âœ…
â”œâ”€â”€ xgb_log.txt               âœ…
â””â”€â”€ orchestrator_log.txt      âœ…

models/week1/
â”œâ”€â”€ rf_optimized.pkl          âœ…
â””â”€â”€ xgb_optimized.pkl         âœ…

data/gold/ml_features/
â””â”€â”€ features_enhanced_v2.parquet  âœ… (65 features)
```

---

## ğŸ’¡ Observations

1. **Vitesse** : Les calculs ont Ã©tÃ© beaucoup plus rapides que prÃ©vu (3 min vs 4-6h)
   - Dataset petit (8k samples)
   - CPU puissant (i7, 12 threads)
   - Optuna trÃ¨s efficace

2. **XGBoost gagnant** : Meilleur que RF de +0.57%
   - Plus rÃ©gulier (learning_rate faible = 0.01)
   - RÃ©gularisation L1/L2 activÃ©e

3. **Gains modestes** : +0.66% sur XGBoost
   - Les hyperparamÃ¨tres n'Ã©taient pas trÃ¨s loin de l'optimal
   - Le vrai gain viendra du stacking + nouvelles features

---

## ğŸ¯ Objectif Semaine 1 (Restant)

**Actuel** : 76.76% (XGBoost)  
**Objectif** : 79-80%  
**Gap** : +2.24% Ã  +3.24%

**Plan pour combler le gap** :
1. âœ… Optimisation (fait)
2. â³ Stacking (en cours)
3. â³ Nouvelles features (Ã  tester)
4. â³ Calibration

---

**Suite** : Je crÃ©e maintenant le stacking avec les modÃ¨les optimisÃ©s !
