# ğŸ“Š MONITORING - NBA Analytics Platform

**Version :** 1.0  
**Date :** 8 FÃ©vrier 2026  
**Statut :** âœ… Production Ready

---

## ğŸ¯ Vue d'Ensemble

SystÃ¨me de monitoring complet pour le pipeline NBA Analytics, implÃ©mentant les stories **NBA-26** (Tests), **NBA-27** (Data Quality) et **NBA-28** (Monitoring).

**Philosophie :** Centraliser les patterns dispersÃ©s plutÃ´t que de dupliquer.

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MONITORING SYSTEM                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   Logging    â”‚  â”‚   Metrics    â”‚  â”‚   Alerts     â”‚     â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚     â”‚
â”‚  â”‚ get_logger() â”‚  â”‚ PipelineMetricsâ”‚  â”‚ AlertManager â”‚     â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚         â”‚                 â”‚                 â”‚              â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                  â”‚                 â”‚                       â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚         â”‚     DataQualityReporter           â”‚              â”‚
â”‚         â”‚  (Validation Bronze/Silver/Gold)  â”‚              â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                           â”‚                                â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚                  â”‚   logs/         â”‚                       â”‚
â”‚                  â”‚   â”œâ”€â”€ metrics/  â”‚                       â”‚
â”‚                  â”‚   â”œâ”€â”€ quality/  â”‚                       â”‚
â”‚                  â”‚   â””â”€â”€ alerts.logâ”‚                       â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Modules

### 1. monitoring.py

**Localisation :** `src/utils/monitoring.py`

**FonctionnalitÃ©s :**
- Logger standardisÃ©
- Collecte de mÃ©triques (timings, volumes, erreurs)
- Validation qualitÃ© donnÃ©es

**Classes principales :**

#### `get_logger(name: str)`
Logger centralisÃ© qui remplace les 5+ configurations dispersÃ©es.

```python
from src.utils import get_logger

logger = get_logger(__name__)
logger.info("Pipeline dÃ©marrÃ©")
logger.error("Erreur dÃ©tectÃ©e")
```

#### `PipelineMetrics`
Collecte les mÃ©triques d'exÃ©cution.

```python
from src.utils import PipelineMetrics

metrics = PipelineMetrics("nba25_pipeline")

# Enregistrer timing
metrics.record_timing("feature_engineering", 2.5)

# Enregistrer volume
metrics.record_volume("predictions", 150)

# Enregistrer erreur
metrics.record_error("APIError", "Timeout NBA API")

# Finaliser et sauvegarder
metrics.finalize("success")
metrics.save_report()  # Sauvegarde dans logs/metrics/
```

**Rapport gÃ©nÃ©rÃ© :**
```json
{
  "pipeline": "nba25_pipeline",
  "start_time": "2026-02-08T20:00:00",
  "timings": {
    "feature_engineering": {
      "duration_seconds": 2.5,
      "timestamp": "2026-02-08T20:00:02"
    }
  },
  "volumes": {
    "predictions": {
      "record_count": 150,
      "timestamp": "2026-02-08T20:00:05"
    }
  },
  "errors": [],
  "status": "success",
  "end_time": "2026-02-08T20:00:10",
  "total_duration_seconds": 10.0
}
```

#### `DataQualityReporter`
Valide la qualitÃ© des donnÃ©es Ã  travers les couches.

```python
from src.utils import DataQualityReporter

reporter = DataQualityReporter()

# Validation complÃ¨te
results = reporter.run_full_check(
    bronze_data=bronze_players,
    silver_data=silver_players,
    gold_data=ml_features
)

# Sauvegarder rapport
reporter.save_report()  # Sauvegarde dans logs/quality/
```

**Rapport gÃ©nÃ©rÃ© :**
```json
{
  "bronze": {
    "record_count": 5103,
    "unique_ids": 5103,
    "duplicates": 0,
    "completion_rate": 0.67,
    "status": "pass"
  },
  "silver": {
    "record_count": 4857,
    "validation_level": "contemporary",
    "null_rate": 0.05,
    "status": "pass"
  },
  "gold": {
    "record_count": 8871,
    "feature_count": 35,
    "nan_rate": 0.001,
    "ml_ready": true,
    "status": "pass"
  },
  "summary": {
    "timestamp": "2026-02-08T20:00:00",
    "overall_status": "pass"
  }
}
```

---

### 2. alerts.py

**Localisation :** `src/utils/alerts.py`

**FonctionnalitÃ©s :**
- SystÃ¨me d'alertes avec logs dÃ©diÃ©s
- Alertes spÃ©cifiques (drift, qualitÃ©, pipeline, performance)

**Classes principales :**

#### `AlertManager`
Gestionnaire d'alertes centralisÃ©.

```python
from src.utils import AlertManager

alerts = AlertManager()
alerts.send_alert("Message", "warning", "mon_module")
```

#### Fonctions helper

```python
from src.utils import (
    alert_on_drift,
    alert_on_quality_failure,
    alert_on_pipeline_failure,
    alert_on_performance_degradation,
    send_alert
)

# Alerte sur drift dÃ©tectÃ©
alert_on_drift("weighted_form_diff", drift_score=0.08, threshold=0.05)

# Alerte sur Ã©chec validation
alert_on_quality_failure("silver_players", ["Taux nulls: 15%"])

# Alerte sur Ã©chec pipeline
alert_on_pipeline_failure("nba22_training", "Out of memory", "model_training")

# Alerte sur dÃ©gradation performance
alert_on_performance_degradation("accuracy", 0.72, 0.76, 5.0)

# Alerte gÃ©nÃ©rique
send_alert("Message personnalisÃ©", "info", "mon_module")
```

**Format logs/alerts.log :**
```
2026-02-08 20:00:00 - WARNING - âš ï¸  [drift_monitoring] Drift dÃ©tectÃ© sur 'weighted_form_diff': score=0.0800 < seuil=0.05
2026-02-08 20:00:01 - ERROR - âŒ [data_quality] Validation qualitÃ© Ã©chouÃ©e pour 'silver_players': Taux nulls: 15%
2026-02-08 20:00:02 - ERROR - âŒ [nba22_training] Pipeline 'nba22_training' Ã©chouÃ© Ã  l'Ã©tape 'model_training': Out of memory
```

---

## ğŸ”§ IntÃ©gration dans les Pipelines

### Enhanced Pipeline (NBA-25)

Le pipeline ML automatisÃ© utilise le monitoring pour tracker les performances.

```python
# Dans enhanced_pipeline.py
from src.utils import PipelineMetrics, alert_on_pipeline_failure, log_pipeline_start, log_pipeline_end

class EnhancedPredictionPipeline:
    def run_auto_pipeline(self, ...):
        log_pipeline_start("NBA-25: PIPELINE ML AUTOMATISÃ‰")
        metrics = PipelineMetrics("nba25_auto_pipeline")
        
        try:
            # Phase 1: Health check
            phase_start = time.time()
            health = self.check_system_health()
            metrics.record_timing("health_check", time.time() - phase_start)
            
            # Phase 2: Predictions
            phase_start = time.time()
            predictions = self.run_daily_predictions()
            metrics.record_timing("predictions", time.time() - phase_start)
            metrics.record_volume("predictions", len(predictions))
            
            # Finalisation
            metrics.finalize("success")
            metrics.save_report()
            log_pipeline_end("NBA-25: PIPELINE ML AUTOMATISÃ‰", "success")
            
        except Exception as e:
            metrics.record_error("PipelineError", str(e))
            metrics.finalize("failure")
            metrics.save_report()
            alert_on_pipeline_failure("nba25_auto_pipeline", str(e), "pipeline_execution")
            log_pipeline_end("NBA-25: PIPELINE ML AUTOMATISÃ‰", "failure")
```

### Drift Monitoring (NBA-22)

Le monitoring de drift dÃ©clenche automatiquement des alertes.

```python
# Dans drift_monitoring.py
from src.utils import alert_on_drift, alert_on_performance_degradation

class DataDriftMonitor:
    def detect_feature_drift(self, ...):
        # ... dÃ©tection drift ...
        
        if result['drift_detected']:
            for feat in drifted_features:
                alert_on_drift(feat['feature'], feat['p_value'], self.alert_threshold)
    
    def check_performance_degradation(self, ...):
        # ... check performance ...
        
        if result['degradation_detected']:
            alert_on_performance_degradation(
                "accuracy", current_acc, global_acc, 10.0
            )
```

---

## ğŸ“Š Dashboard Monitoring

### Voir les mÃ©triques

```bash
# Lister tous les rapports de mÃ©triques
ls -lt logs/metrics/

# Voir le dernier rapport
cat logs/metrics/$(ls -t logs/metrics/ | head -1)

# Suivre en temps rÃ©el
tail -f logs/metrics/*.json
```

### Voir les alertes

```bash
# Voir toutes les alertes
cat logs/alerts.log

# Voir les alertes rÃ©centes
tail -n 50 logs/alerts.log

# Suivre en temps rÃ©el
tail -f logs/alerts.log

# Filtrer par sÃ©vÃ©ritÃ©
grep "ERROR" logs/alerts.log
grep "WARNING" logs/alerts.log
```

### Voir les rapports qualitÃ©

```bash
# Lister les rapports qualitÃ©
ls -lt logs/quality/

# Voir le dernier rapport
cat logs/quality/$(ls -t logs/quality/ | head -1)
```

---

## âš™ï¸ Configuration

### monitoring.yaml

**Localisation :** `configs/monitoring.yaml`

**Sections principales :**

```yaml
monitoring:
  logging:
    level: INFO
    format: json
    rotation: daily
    retention_days: 30
    
  metrics:
    enabled: true
    collect_timing: true
    collect_volume: true
    thresholds:
      max_pipeline_duration_seconds: 3600
      max_error_rate: 0.05
      
  quality:
    enabled: true
    thresholds:
      bronze:
        min_completion_rate: 0.30
      silver:
        max_null_rate: 0.20
      gold:
        max_nan_rate: 0.01
        
  alerts:
    enabled: true
    channels:
      log:
        enabled: true
        file: logs/alerts.log
      console:
        enabled: true
        min_severity: error
```

---

## ğŸ§ª Tests

### Tests ML Pipeline

**Fichier :** `tests/test_ml_pipeline_critical.py`

```bash
# ExÃ©cuter tous les tests
pytest tests/test_ml_pipeline_critical.py -v

# ExÃ©cuter une classe de tests spÃ©cifique
pytest tests/test_ml_pipeline_critical.py::TestOptimizedTrainer -v

# ExÃ©cuter un test spÃ©cifique
pytest tests/test_ml_pipeline_critical.py::TestOptimizedTrainer::test_trainer_initialization -v
```

**Couverture :**
- EntraÃ®nement optimisÃ©
- DÃ©tection de drift
- Calibration des probabilitÃ©s
- SÃ©lection de features
- Pipeline quotidien
- Flux end-to-end

---

## ğŸ“ˆ MÃ©triques ClÃ©s

### Performance Pipeline

| MÃ©trique | Seuil d'alerte | Description |
|----------|----------------|-------------|
| Pipeline duration | > 1h | Temps total d'exÃ©cution |
| Phase timing | Variable | Temps par Ã©tape |
| Error rate | > 5% | Taux d'erreurs |

### QualitÃ© DonnÃ©es

| Couche | ComplÃ©tion | Nulls | Statut |
|--------|------------|-------|--------|
| Bronze | > 30% | < 70% | âœ“ |
| Silver | > 80% | < 20% | âœ“ |
| Gold | > 95% | < 1% | âœ“ |

### Performance ML

| MÃ©trique | Baseline | Seuil alerte |
|----------|----------|--------------|
| Accuracy | 76.76% | -5% |
| AUC | 84.93% | -3% |
| Brier Score | 0.15 | +10% |

---

## ğŸš¨ Alertes Importantes

### Drift DÃ©tectÃ©
**Cause :** Distribution des features change significativement  
**Action :** RÃ©entraÃ®ner le modÃ¨le avec nouvelles donnÃ©es

### Performance DÃ©gradÃ©e
**Cause :** Accuracy baisse de > 5% vs baseline  
**Action :** VÃ©rifier data quality, rÃ©entraÃ®ner si nÃ©cessaire

### Pipeline Failure
**Cause :** Exception non gÃ©rÃ©e dans le pipeline  
**Action :** VÃ©rifier logs, corriger erreur, relancer

### Quality Check Failed
**Cause :** DonnÃ©es ne respectent pas les seuils qualitÃ©  
**Action :** VÃ©rifier source donnÃ©es, corriger anomalies

---

## ğŸ”— Ressources

- **Code source :** `src/utils/monitoring.py`, `src/utils/alerts.py`
- **Tests :** `tests/test_ml_pipeline_critical.py`
- **Configuration :** `configs/monitoring.yaml`
- **Documentation API :** `src/utils/__init__.py`

---

**DerniÃ¨re mise Ã  jour :** 8 FÃ©vrier 2026  
**Version :** 1.0  
**Auteur :** Agent/Data Engineer
