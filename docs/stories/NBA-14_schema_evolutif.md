---
Story: NBA-14
Epic: Data Ingestion & Collection (NBA-6)
Points: 5
Statut: In Progress
Priorit√©: Medium
Assign√©: Isaak
Cr√©√©: 05/Feb/26
---

# üéØ NBA-14: Gestion des sch√©mas √©volutifs

## üìã Description

G√©rer les changements de sch√©ma dans les donn√©es NBA avec Delta Lake. Impl√©menter un syst√®me de versioning des sch√©mas qui permet d'√©voluer sans casser les traitements existants.

## üîó D√©pendances

### D√©pend de:
- ‚úÖ **NBA-12** : Pipeline Spark batch (Delta Lake d√©j√† en place)
- ‚úÖ **NBA-13** : Streaming (gestion temps r√©el)

### Bloque:
- ‚¨ú **NBA-17** : Nettoyage (n√©cessite sch√©ma stable)
- ‚¨ú **NBA-18** : M√©triques avanc√©es (d√©pend du sch√©ma final)
- ‚¨ú **NBA-20** : Transformation matchs (structure des donn√©es)

### Parall√®le avec:
- ‚¨ú **NBA-15** : R√©cup√©ration donn√©es (doit respecter sch√©ma)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ NBA-12  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ NBA-14  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ NBA-17  ‚îÇ
‚îÇ (Delta) ‚îÇ     ‚îÇ(Sch√©mas)‚îÇ     ‚îÇ(Clean)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚Üí NBA-18 (M√©triques)
                     ‚îÇ
                     ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚Üí NBA-20 (Transform)
                     ‚îÇ
                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚Üí NBA-15 (parallel)
```

## üì•üì§ Entr√©es/Sorties

### Donn√©es en entr√©e:
- **`data/processed/games_enriched/`** : Delta Lake existant (NBA-12)
- **Structure actuelle** : 20+ colonnes (PTS, REB, AST, PER, TS%, etc.)

### Donn√©es en sortie:
- **`data/processed/games_enriched/`** : M√™me chemin avec MergeSchema
- **`data/processed/schema_versions/`** : Historique des versions
- **`docs/schema_evolution.log`** : Log des changements

### Format:
- **Format**: Delta Lake 3.0
- **Partitionnement**: `season`, `game_year` (conserv√©)
- **Versioning**: Time travel Delta Lake + m√©tadonn√©es custom

## üõ†Ô∏è Stack Technique

- **PySpark 3.5** : Lectures/√©critures Delta
- **Delta Lake 3.0** : MergeSchema, time travel
- **Python 3.11** : Script de gestion
- **PyYAML** : Configuration sch√©mas

### Biblioth√®ques:
```python
from delta import DeltaTable, configure_spark_with_delta_pip
from pyspark.sql.functions import lit, current_timestamp
import yaml
import json
```

## ‚úÖ Crit√®res d'acceptation d√©taill√©s

### 1. MergeSchema activ√© sur les √©critures Delta

**Test d√©taill√©:**
```python
# DONN√âES TEST
df_old = spark.createDataFrame([
    ("2024-01-01", "LAL", 120, None),      # Sans nouvelle colonne
    ("2024-01-02", "GSW", 115, None)
], ["date", "team", "points", "new_metric"])

# √âCRITURE AVEC MERGESCHEMA
df_old.write \
    .format("delta") \
    .option("mergeSchema", "true") \
    .mode("append") \
    .save("data/processed/games_enriched/")

# V√âRIFICATION
schema = spark.read.format("delta").load("data/processed/games_enriched/").schema
assert "new_metric" in [f.name for f in schema.fields]
print("‚úÖ MergeSchema fonctionne!")
```

**R√©sultat attendu:**
- Sch√©ma √©volutif sans erreur
- Colonnes manquantes = null
- Anciennes donn√©es conserv√©es

---

### 2. Versioning des sch√©mas fonctionnel

**Test d√©taill√©:**
```python
# TEST TIME TRAVEL
from delta import DeltaTable

dt = DeltaTable.forPath(spark, "data/processed/games_enriched/")
history = dt.history()

# V√©rifier qu'on a au moins 2 versions
assert len(history) >= 2, "Besoin d'historique pour test"

# Lire version N-1
df_v1 = spark.read \
    .format("delta") \
    .option("versionAsOf", 0) \
    .load("data/processed/games_enriched/")

# Lire version N (actuelle)
df_v2 = spark.read \
    .format("delta") \
    .load("data/processed/games_enriched/")

# V√©rifier diff√©rences
old_cols = set(df_v1.columns)
new_cols = set(df_v2.columns)
added_cols = new_cols - old_cols

print(f"‚úÖ Colonnes ajout√©es: {added_cols}")
print(f"‚úÖ Time travel fonctionne: {len(history)} versions")
```

**R√©sultat attendu:**
- Lecture version historique possible
- Diff√©rence colonnes identifiable
- M√©tadonn√©es de changement pr√©sentes

---

### 3. Test de changement de sch√©ma r√©ussi

**Sc√©nario de test complet:**

**√âtape 1: Sch√©ma initial (V1)**
```python
# Cr√©er donn√©es V1
df_v1 = spark.createDataFrame([
    (1, "LAL", 120.0, 45),
    (2, "GSW", 115.0, 42)
], ["game_id", "team", "points", "rebounds"])

df_v1.write.format("delta").mode("overwrite") \
    .save("data/processed/test_schema/")
```

**√âtape 2: Sch√©ma √©volutif (V2)**
```python
# Ajouter colonnes
df_v2 = spark.createDataFrame([
    (3, "BOS", 108.0, 38, 25, 0.58),  # + assists, + ts_pct
    (4, "MIA", 112.0, 41, 22, 0.62)
], ["game_id", "team", "points", "rebounds", "assists", "ts_pct"])

df_v2.write.format("delta") \
    .option("mergeSchema", "true") \
    .mode("append") \
    .save("data/processed/test_schema/")
```

**√âtape 3: V√©rifications**
```python
# Lire toutes les donn√©es
df_all = spark.read.format("delta").load("data/processed/test_schema/")

# V√©rifications:
assert df_all.count() == 4, "Toutes les lignes pr√©sentes"
assert "assists" in df_all.columns, "Nouvelle colonne ajout√©e"
assert "ts_pct" in df_all.columns, "Nouvelle colonne ajout√©e"

# V√©rifier nulls pour anciennes donn√©es
v1_data = df_all.filter(col("game_id").isin([1, 2]))
assert v1_data.filter(col("assists").isNull()).count() == 2, "Anciennes donn√©es = null"

print("‚úÖ Changement de sch√©ma r√©ussi!")
print(f"   - Total lignes: {df_all.count()}")
print(f"   - Total colonnes: {len(df_all.columns)}")
print(f"   - Colonnes: {df_all.columns}")
```

**R√©sultat attendu:**
- 4 lignes (2 V1 + 2 V2)
- 6 colonnes (4 originales + 2 nouvelles)
- V1: assists=null, ts_pct=null
- V2: valeurs renseign√©es

---

### 4. Documentation des √©volutions de sch√©ma

**Livrable:** `docs/schema_evolution.log`

**Format attendu:**
```yaml
schema_versions:
  - version: 1
    date: "2024-02-06T10:30:00"
    columns: ["game_id", "team", "points", "rebounds"]
    nb_records: 8600
    
  - version: 2
    date: "2024-02-06T11:15:00"
    columns: ["game_id", "team", "points", "rebounds", "assists", "ts_pct"]
    nb_records: 8602
    changes:
      added: ["assists", "ts_pct"]
      removed: []
      modified: []
    author: "NBA-14"
```

**Test:**
```python
import yaml

with open("docs/schema_evolution.log") as f:
    history = yaml.safe_load(f)

assert len(history["schema_versions"]) >= 2
assert "changes" in history["schema_versions"][-1]
print("‚úÖ Documentation √† jour!")
```

## ‚ö†Ô∏è Risques & Mitigations

| Risque | Probabilit√© | Impact | Mitigation |
|--------|-------------|--------|------------|
| **Breaking change** | Faible | Critique | Tests sur environnement isol√© avant prod |
| **Donn√©es corrompues** | Faible | Critique | Backup automatique avant migration |
| **Performance d√©grad√©e** | Moyen | Moyen | Monitoring temps requ√™tes, optimise si besoin |
| **Incoh√©rence m√©trique** | Moyen | √âlev√© | Validation donn√©es apr√®s changement (NBA-27) |
| **Rollback impossible** | Faible | √âlev√© | Time travel Delta = rollback toujours possible |

### Plan de secours:
1. Backup automatique: `cp -r data/processed/ data/backup/$(date +%Y%m%d)/`
2. Rollback: `spark.read.format("delta").option("versionAsOf", N-1).load(...)`
3. Hotfix: Script de correction rapide

## üì¶ Livrables

### Code:
- ‚úÖ `src/utils/schema_manager.py` - Gestionnaire de sch√©mas
- ‚úÖ `src/utils/schema_config.yaml` - Configuration sch√©mas
- ‚úÖ `tests/test_schema_evolution.py` - Tests unitaires

### Documentation:
- ‚úÖ `docs/schema_evolution.log` - Historique versions
- ‚úÖ `docs/SCHEMA_VERSIONING.md` - Guide d'utilisation

### Donn√©es:
- ‚úÖ `data/processed/games_enriched/` - Delta Lake avec MergeSchema activ√©
- ‚úÖ `data/processed/schema_versions/` - Backup versions

## üéØ Definition of Done

- [x] Code review effectu√©
- [ ] Tests passants (pytest tests/test_schema_evolution.py)
- [ ] Documentation √† jour
- [ ] Pas de r√©gression sur NBA-12/NBA-13
- [ ] Performance acceptable (< 10% d√©gradation)
- [ ] Merg√© dans master (PR #X)

## üìù Notes d'impl√©mentation

### Activation MergeSchema:
```python
# Option globale
spark.conf.set("spark.databricks.delta.schema.autoMerge.enabled", "true")

# Option par √©criture
df.write.format("delta").option("mergeSchema", "true").mode("append").save(...)
```

### V√©rification version:
```python
from delta import DeltaTable
dt = DeltaTable.forPath(spark, path)
print(dt.history().select("version", "timestamp", "operation").show())
```

## üîó R√©f√©rences

- [Delta Lake Schema Evolution](https://docs.delta.io/latest/delta-update.html#automatic-schema-update)
- [Time Travel](https://docs.delta.io/latest/delta-batch.html#data-versioning)
- NBA-12: Pipeline batch existant
- NBA-13: Streaming avec Delta
