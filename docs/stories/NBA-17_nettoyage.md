---
Story: NBA-17
Epic: Data Processing & Transformation (NBA-7)
Points: 5
Statut: To Do
PrioritÃ©: Medium
AssignÃ©: Isaak
CrÃ©Ã©: 05/Feb/26
---

# ğŸ¯ NBA-17: Nettoyage des donnÃ©es joueurs

## ğŸ“‹ Description

Nettoyer les donnÃ©es brutes des joueurs (nulls, doublons, valeurs aberrantes) pour produire un dataset propre et fiable dans la couche Silver.

## ğŸ”— DÃ©pendances

### DÃ©pend de:
- âœ… **NBA-11** : DonnÃ©es brutes joueurs
- âœ… **NBA-15** : DonnÃ©es complÃ¨tes (matchs/Ã©quipes)
- ğŸŸ¡ **NBA-14** : SchÃ©mas Ã©volutifs

### Bloque:
- â¬œ **NBA-18** : MÃ©triques avancÃ©es (besoin donnÃ©es propres)
- â¬œ **NBA-19** : AgrÃ©gations (qualitÃ© requise)
- â¬œ **NBA-21** : Feature engineering (donnÃ©es nettoyÃ©es)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ NBA-15  â”‚â”€â”€â”€â”€â†’â”‚ NBA-17  â”‚â”€â”€â”€â”€â†’â”‚ NBA-18  â”‚
â”‚(DonnÃ©es)â”‚     â”‚(Clean)  â”‚     â”‚(MÃ©triques)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”œâ”€â”€â”€â”€â†’ NBA-19 (AggrÃ©g)
                     â”‚
                     â””â”€â”€â”€â”€â†’ NBA-21 (Features)
```

## ğŸ“¥ğŸ“¤ EntrÃ©es/Sorties

### DonnÃ©es en entrÃ©e:
- **`data/raw/players.json`** : DonnÃ©es brutes (5103 joueurs)
- **`data/raw/all_players_historical.json`** : Historique complet
- **`data/raw/active_players.json`** : Joueurs actifs (530)

### DonnÃ©es en sortie:
- **`data/silver/players_cleaned/`** : Delta Lake nettoyÃ©
- **`data/silver/players_cleaned_stats.json`** : Rapport nettoyage
- **`logs/cleaning_YYYYMMDD.log`** : Log dÃ©taillÃ© opÃ©rations

### Format:
- **Format**: Delta Lake partitionnÃ© par `is_active`, `position`
- **QualitÃ©**: Taux de nulls < 5%, 0 doublons

## ğŸ› ï¸ Stack Technique

- **PySpark 3.5** : DataFrame operations
- **Delta Lake 3.0** : Stockage Silver
- **Pandas** : Analyse exploratoire (optionnel)
- **Great Expectations** : Data quality (optionnel)

## âœ… CritÃ¨res d'acceptation dÃ©taillÃ©s

### 1. Script src/processing/clean_data.py crÃ©Ã©

**Structure du script:**
```python
#!/usr/bin/env python3
"""
Script de nettoyage des donnÃ©es joueurs NBA
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, when, isnan, count, lit, 
    trim, lower, regexp_replace
)
from delta import configure_spark_with_delta_pip
import logging
import json
from datetime import datetime

class PlayersDataCleaner:
    def __init__(self, input_path, output_path):
        self.spark = self._init_spark()
        self.input_path = input_path
        self.output_path = output_path
        self.stats = {}
        
    def _init_spark(self):
        """Initialiser session Spark avec Delta"""
        builder = SparkSession.builder \
            .appName("NBA-Players-Cleaning") \
            .config("spark.sql.extensions", 
                   "io.delta.sql.DeltaSparkSessionExtension")
        return configure_spark_with_delta_pip(builder).getOrCreate()
    
    def load_data(self):
        """Charger donnÃ©es brutes"""
        self.df_raw = self.spark.read.json(self.input_path)
        self.stats['initial_count'] = self.df_raw.count()
        self.stats['initial_columns'] = len(self.df_raw.columns)
        return self
    
    def remove_duplicates(self):
        """Supprimer doublons basÃ©s sur player_id"""
        initial = self.df_raw.count()
        self.df_clean = self.df_raw.dropDuplicates(["id"])
        final = self.df_clean.count()
        self.stats['duplicates_removed'] = initial - final
        return self
    
    def handle_nulls(self):
        """GÃ©rer valeurs manquantes"""
        # Colonnes critiques (doivent Ãªtre non-null)
        critical_cols = ["id", "full_name"]
        self.df_clean = self.df_clean.dropna(subset=critical_cols)
        
        # Colonnes numÃ©riques (imputation ou suppression)
        numeric_cols = ["height", "weight", "pts", "reb", "ast"]
        for col_name in numeric_cols:
            null_count = self.df_clean.filter(col(col_name).isNull()).count()
            if null_count > 0:
                # Si >50% null, supprimer colonne
                if null_count / self.df_clean.count() > 0.5:
                    self.df_clean = self.df_clean.drop(col_name)
                    self.stats[f'{col_name}_dropped'] = True
                else:
                    # Sinon imputer avec mÃ©diane
                    median = self.df_clean.approxQuantile(
                        col_name, [0.5], 0.01
                    )[0]
                    self.df_clean = self.df_clean.fillna({col_name: median})
                    self.stats[f'{col_name}_imputed'] = median
        
        return self
    
    def remove_outliers(self):
        """Supprimer valeurs aberrantes"""
        # Taille: entre 1.60m et 2.40m
        self.df_clean = self.df_clean.filter(
            (col("height") >= 160) & (col("height") <= 240) |
            col("height").isNull()
        )
        
        # Poids: entre 60kg et 160kg
        self.df_clean = self.df_clean.filter(
            (col("weight") >= 60) & (col("weight") <= 160) |
            col("weight").isNull()
        )
        
        # Stats: pas de valeurs nÃ©gatives
        stat_cols = ["pts", "reb", "ast", "stl", "blk"]
        for col_name in stat_cols:
            self.df_clean = self.df_clean.filter(
                (col(col_name) >= 0) | col(col_name).isNull()
            )
        
        return self
    
    def standardize_formats(self):
        """Standardiser formats"""
        # Nom: majuscule premiÃ¨re lettre
        self.df_clean = self.df_clean.withColumn(
            "full_name", 
            trim(col("full_name"))
        )
        
        # Position: majuscules
        self.df_clean = self.df_clean.withColumn(
            "position",
            upper(col("position"))
        )
        
        return self
    
    def validate_data(self):
        """Valider qualitÃ© donnÃ©es"""
        # Calculer taux de nulls par colonne
        null_rates = {}
        for col_name in self.df_clean.columns:
            null_count = self.df_clean.filter(col(col_name).isNull()).count()
            null_rate = null_count / self.df_clean.count()
            null_rates[col_name] = null_rate
        
        self.stats['null_rates'] = null_rates
        
        # VÃ©rifier taux global < 5%
        overall_null = sum(null_rates.values()) / len(null_rates)
        assert overall_null < 0.05, f"Taux nulls trop Ã©levÃ©: {overall_null:.2%}"
        
        return self
    
    def save_clean_data(self):
        """Sauvegarder donnÃ©es nettoyÃ©es"""
        self.df_clean.write \
            .format("delta") \
            .mode("overwrite") \
            .partitionBy("is_active", "position") \
            .save(self.output_path)
        
        self.stats['final_count'] = self.df_clean.count()
        self.stats['final_columns'] = len(self.df_clean.columns)
        
        return self
    
    def generate_report(self):
        """GÃ©nÃ©rer rapport nettoyage"""
        report = {
            "timestamp": datetime.now().isoformat(),
            "input": self.input_path,
            "output": self.output_path,
            "stats": self.stats
        }
        
        with open("data/silver/players_cleaned_stats.json", "w") as f:
            json.dump(report, f, indent=2)
        
        return report

# Point d'entrÃ©e
if __name__ == "__main__":
    cleaner = PlayersDataCleaner(
        input_path="data/raw/all_players_historical.json",
        output_path="data/silver/players_cleaned"
    )
    
    report = (cleaner
        .load_data()
        .remove_duplicates()
        .handle_nulls()
        .remove_outliers()
        .standardize_formats()
        .validate_data()
        .save_clean_data()
        .generate_report()
    )
    
    print(f"âœ… Nettoyage terminÃ©!")
    print(f"   - Initial: {report['stats']['initial_count']} joueurs")
    print(f"   - Final: {report['stats']['final_count']} joueurs")
    print(f"   - Doublons supprimÃ©s: {report['stats']['duplicates_removed']}")
```

---

### 2. Suppression des doublons

**Test dÃ©taillÃ©:**
```python
def test_duplicate_removal():
    from pyspark.sql import SparkSession
    spark = SparkSession.builder.getOrCreate()
    
    # DonnÃ©es test avec doublons
    data = [
        (1, "LeBron James", "LAL", 39),
        (1, "LeBron James", "LAL", 39),  # Doublon
        (2, "Stephen Curry", "GSW", 35),
        (2, "Stephen Curry", "GSW", 35),  # Doublon
        (3, "Kevin Durant", "PHX", 35)
    ]
    
    df = spark.createDataFrame(data, ["id", "name", "team", "age"])
    initial = df.count()
    
    # Supprimer doublons
    df_clean = df.dropDuplicates(["id"])
    final = df_clean.count()
    
    assert initial == 5, f"DonnÃ©es initiales: {initial}"
    assert final == 3, f"Doublons non supprimÃ©s: {final}"
    assert df_clean.filter(col("id") == 1).count() == 1, "Doublon ID=1 persistant"
    
    print(f"âœ… Doublons supprimÃ©s: {initial - final} lignes")
    return True

test_duplicate_removal()
```

**RÃ©sultat attendu:**
- 0 doublons sur `id` joueur
- Journal: nombre de doublons dÃ©tectÃ©s et supprimÃ©s

---

### 3. Taux de nulls < 5% aprÃ¨s traitement

**Test dÃ©taillÃ©:**
```python
def test_null_rate():
    # AprÃ¨s nettoyage
    df_clean = spark.read.format("delta").load("data/silver/players_cleaned/")
    
    total_rows = df_clean.count()
    null_rates = {}
    
    for col_name in df_clean.columns:
        null_count = df_clean.filter(col(col_name).isNull()).count()
        null_rate = null_count / total_rows
        null_rates[col_name] = null_rate
        
        print(f"{col_name}: {null_rate:.2%} nulls")
    
    # VÃ©rifier chaque colonne < 5%
    for col_name, rate in null_rates.items():
        assert rate < 0.05, f"{col_name}: {rate:.2%} nulls > 5%!"
    
    # Taux global
    overall = sum(null_rates.values()) / len(null_rates)
    assert overall < 0.05, f"Taux global: {overall:.2%} > 5%!"
    
    print(f"âœ… Taux nulls global: {overall:.2%}")
    return True

test_null_rate()
```

**RÃ©sultat attendu:**
- Chaque colonne: < 5% nulls
- Taux global: < 5%
- Rapport JSON avec dÃ©tail par colonne

---

### 4. Validation des tailles/poids cohÃ©rents

**RÃ¨gles de validation:**
```python
VALIDATION_RULES = {
    "height": {"min": 160, "max": 240, "unit": "cm"},      # 1.60m - 2.40m
    "weight": {"min": 60, "max": 160, "unit": "kg"},       # 60kg - 160kg
    "pts": {"min": 0, "max": 50, "unit": "points/match"},  # 0-50 PPG
    "reb": {"min": 0, "max": 20, "unit": "reb/match"},     # 0-20 RPG
    "ast": {"min": 0, "max": 15, "unit": "ast/match"},     # 0-15 APG
}
```

**Test dÃ©taillÃ©:**
```python
def test_value_ranges():
    df = spark.read.format("delta").load("data/silver/players_cleaned/")
    
    errors = []
    
    # Tester chaque rÃ¨gle
    for col_name, rules in VALIDATION_RULES.items():
        if col_name in df.columns:
            # Valeurs hors limites
            outliers = df.filter(
                (col(col_name) < rules["min"]) | 
                (col(col_name) > rules["max"])
            )
            
            count = outliers.count()
            if count > 0:
                errors.append(f"{col_name}: {count} valeurs hors limites")
                print(f"âŒ {col_name}: {count} outliers")
                outliers.show(5)
    
    assert len(errors) == 0, f"Erreurs validation: {errors}"
    
    print("âœ… Toutes les valeurs dans les plages valides")
    return True

test_value_ranges()
```

**Exemples de valeurs aberrantes dÃ©tectÃ©es:**
- Taille: 0cm, 300cm, -180cm
- Poids: 0kg, 250kg, -90kg
- Points: -5, 100

---

### 5. DonnÃ©es nettoyÃ©es dans data/silver/players_cleaned

**VÃ©rification structure:**
```python
def test_silver_structure():
    import os
    
    path = "data/silver/players_cleaned"
    
    # VÃ©rifier Delta Lake
    assert os.path.exists(f"{path}/_delta_log"), "Pas un Delta Lake!"
    
    # VÃ©rifier partitionnement
    partitions = [d for d in os.listdir(path) if d.startswith("is_active")]
    assert len(partitions) >= 2, f"Partitionnement incorrect: {partitions}"
    
    # Lire et vÃ©rifier
    df = spark.read.format("delta").load(path)
    
    # VÃ©rifier colonnes clÃ©s
    required_cols = ["id", "full_name", "is_active", "position"]
    for col_name in required_cols:
        assert col_name in df.columns, f"Colonne {col_name} manquante!"
    
    # VÃ©rifier nombre joueurs cohÃ©rent
    count = df.count()
    assert 5000 < count < 5100, f"Nombre joueurs incohÃ©rent: {count}"
    
    print(f"âœ… Structure Silver correcte: {count} joueurs")
    return True

test_silver_structure()
```

## âš ï¸ Risques & Mitigations

| Risque | ProbabilitÃ© | Impact | Mitigation |
|--------|-------------|--------|------------|
| **Perte donnÃ©es critiques** | Faible | Critique | Backup avant nettoyage, tests validation |
| **Over-cleaning** | Moyen | Moyen | Sauvegarder donnÃ©es brutes, log dÃ©taillÃ© |
| **RÃ¨gles trop strictes** | Moyen | Moyen | Review rÃ¨gles mÃ©tier, exceptions documentÃ©es |
| **Performance lente** | Moyen | Faible | Partitionnement, caching, monitoring temps |

## ğŸ“¦ Livrables

### Code:
- âœ… `src/processing/clean_data.py` - Script principal
- âœ… `src/processing/cleaning_utils.py` - Fonctions utilitaires
- âœ… `tests/test_cleaning.py` - Tests unitaires
- âœ… `configs/cleaning_rules.yaml` - RÃ¨gles de validation

### DonnÃ©es:
- âœ… `data/silver/players_cleaned/` - Delta Lake nettoyÃ©
- âœ… `data/silver/players_cleaned_stats.json` - Rapport qualitÃ©
- âœ… `logs/cleaning_YYYYMMDD.log` - Logs dÃ©taillÃ©s

### Documentation:
- âœ… `docs/DATA_CLEANING.md` - Guide nettoyage

## ğŸ¯ Definition of Done

- [ ] Script clean_data.py exÃ©cutable sans erreur
- [ ] 0 doublons dans donnÃ©es finales
- [ ] Taux nulls < 5% global
- [ ] Toutes les valeurs dans plages valides
- [ ] Rapport JSON gÃ©nÃ©rÃ©
- [ ] Tests passants (pytest tests/test_cleaning.py)
- [ ] MergÃ© dans master (PR #X)

## ğŸ“ Notes d'implÃ©mentation

### Great Expectations (optionnel):
```python
# Pour data quality avancÃ©e
import great_expectations as ge

# DÃ©finir expectations
batch = ge.dataset.SparkDFDataset(df)
batch.expect_column_values_to_be_between("height", 160, 240)
batch.expect_column_values_to_not_be_null("full_name")

# Validation
results = batch.validate()
assert results["success"], "Data quality checks failed"
```

### Monitoring qualitÃ©:
```python
def log_data_quality(df, stage):
    """Log mÃ©triques qualitÃ©"""
    metrics = {
        "stage": stage,
        "row_count": df.count(),
        "null_rates": {c: df.filter(col(c).isNull()).count()/df.count() 
                      for c in df.columns},
        "timestamp": datetime.now().isoformat()
    }
    
    with open(f"logs/quality_{stage}.json", "w") as f:
        json.dump(metrics, f)
```

## ğŸ”— RÃ©fÃ©rences

- [NBA-11](NBA-11_api_connection.md) : DonnÃ©es brutes
- [NBA-15](NBA-15_donnees_matchs.md) : DonnÃ©es complÃ¨tes
- [NBA-27](NBA-27_data_quality.md) : Data Quality checks
