---
Story: NBA-19
Epic: Data Processing & Transformation (NBA-7)
Points: 3
Statut: âœ… Done
PrioritÃ©: Medium
AssignÃ©: Isaak
CrÃ©Ã©: 05/Feb/26
TerminÃ©: 08/Feb/26
---

# ğŸ¯ NBA-19: AgrÃ©gations par Ã©quipe et saison

## ğŸ“‹ Description

CrÃ©er des agrÃ©gations Spark SQL des statistiques par Ã©quipe et par saison, avec jointures joueurs-Ã©quipes et optimisation des requÃªtes SQL.

## ğŸ”— DÃ©pendances

### DÃ©pend de:
- âœ… **NBA-17** : DonnÃ©es nettoyÃ©es
- âœ… **NBA-18** : MÃ©triques avancÃ©es
- â¬œ **NBA-15** : DonnÃ©es Ã©quipes complÃ¨tes

### Bloque:
- â¬œ **NBA-22** : ML (besoin stats Ã©quipes)
- â¬œ **NBA-29** : Export BI (donnÃ©es agrÃ©gÃ©es)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ NBA-18  â”‚â”€â”€â”€â”€â†’â”‚ NBA-19  â”‚â”€â”€â”€â”€â†’â”‚ NBA-22  â”‚
â”‚(MÃ©triques)
â”‚         â”‚     â”‚(AggrÃ©g) â”‚     â”‚(ML)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â””â”€â”€â”€â”€â†’ NBA-29 (Export)
```

## ğŸ“¥ğŸ“¤ EntrÃ©es/Sorties

### DonnÃ©es en entrÃ©e:
- **`data/silver/players_advanced/`** : Joueurs avec mÃ©triques (NBA-18)
- **`data/raw/teams_stats/`** : Stats collectives Ã©quipes (NBA-15)
- **`data/raw/teams/teams_2024_25.json`** : Informations Ã©quipes

### DonnÃ©es en sortie:
- **`data/gold/team_stats_season/`** : Delta Lake agrÃ©gations
- **`data/gold/player_team_join/`** : Jointures optimisÃ©es

### Format:
- **AgrÃ©gations**: Moyennes, sommes, classements par saison
- **Jointures**: Joueurs-Ã©quipes avec foreign keys

## ğŸ› ï¸ Stack Technique

- **PySpark SQL** : AgrÃ©gations, jointures
- **Delta Lake** : Stockage couche Gold
- **Window Functions** : Classements, rangs

## âœ… CritÃ¨res d'acceptation dÃ©taillÃ©s

### 1. DataFrame Ã©quipes crÃ©Ã© avec stats agrÃ©gÃ©es

**AgrÃ©gations requises:**
```python
from pyspark.sql.functions import (
    avg, sum, count, max, min, col,
    row_number, rank, dense_rank
)
from pyspark.sql.window import Window

def create_team_aggregates():
    """CrÃ©er agrÃ©gations par Ã©quipe et saison"""
    
    # Lire donnÃ©es
    df_players = spark.read.format("delta").load("data/silver/players_advanced/")
    
    # AgrÃ©gations par Ã©quipe/saison
    df_team_stats = (df_players
        .groupBy("team_id", "team_name", "season")
        .agg(
            # Stats collectives
            avg("pts").alias("avg_pts_scored"),
            sum("pts").alias("total_pts_scored"),
            avg("reb").alias("avg_reb"),
            avg("ast").alias("avg_ast"),
            
            # EfficacitÃ©
            avg("ts_pct").alias("team_ts_pct"),
            avg("per").alias("avg_player_per"),
            max("per").alias("best_player_per"),
            
            # Effectif
            count("*").alias("roster_size"),
            count(when(col("is_active"), True)).alias("active_players"),
            
            # Performance
            avg("minutes").alias("avg_minutes"),
            sum("minutes").alias("total_minutes")
        )
    )
    
    return df_team_stats
```

**Test:**
```python
def test_team_aggregates():
    df = create_team_aggregates()
    
    # VÃ©rifier 30 Ã©quipes
    team_count = df.select("team_id").distinct().count()
    assert team_count == 30, f"Nombre Ã©quipes: {team_count} (attendu: 30)"
    
    # VÃ©rifier colonnes
    required_cols = [
        "team_id", "team_name", "season",
        "avg_pts_scored", "avg_reb", "avg_ast",
        "team_ts_pct", "roster_size"
    ]
    
    for col_name in required_cols:
        assert col_name in df.columns, f"Colonne {col_name} manquante"
    
    # VÃ©rifier valeurs cohÃ©rentes
    stats = df.select(
        avg("avg_pts_scored").alias("avg_team_pts"),
        avg("roster_size").alias("avg_roster")
    ).collect()[0]
    
    assert 100 < stats["avg_team_pts"] < 130, f"Points moyens incohÃ©rents: {stats['avg_team_pts']}"
    assert 12 < stats["avg_roster"] < 18, f"Taille roster incohÃ©rente: {stats['avg_roster']}"
    
    print(f"âœ… {team_count} Ã©quipes avec agrÃ©gations")
    return True
```

---

### 2. Moyennes par saison calculÃ©es

**Moyennes ligue par saison:**
```python
def calculate_season_averages():
    """Calculer moyennes globales par saison"""
    
    df_players = spark.read.format("delta").load("data/silver/players_advanced/")
    
    df_season_avg = (df_players
        .groupBy("season")
        .agg(
            avg("pts").alias("lg_avg_pts"),
            avg("reb").alias("lg_avg_reb"),
            avg("ast").alias("lg_avg_ast"),
            avg("per").alias("lg_avg_per"),
            avg("ts_pct").alias("lg_avg_ts_pct"),
            count("*").alias("total_players"),
            avg("minutes").alias("lg_avg_minutes")
        )
        .orderBy("season")
    )
    
    return df_season_avg

# Test
season_stats = calculate_season_averages()
season_stats.show()
```

**RÃ©sultat attendu:**
```
+----------+----------+----------+----------+---------+-------------+--------------+
|    season|lg_avg_pts|lg_avg_reb|lg_avg_ast|lg_avg_per|total_players|lg_avg_minutes|
+----------+----------+----------+----------+----------+-------------+--------------+
|   2018-19|     12.5|      5.2|      3.1|     15.2|          492|         20.5|
|   2019-20|     12.8|      5.3|      3.2|     15.4|          485|         21.0|
|   2020-21|     13.1|      5.4|      3.3|     15.6|          498|         21.2|
+----------+----------+----------+----------+----------+-------------+--------------+
```

---

### 3. Jointures joueurs-Ã©quipes fonctionnelles

**Jointure optimisÃ©e:**
```python
def create_player_team_join():
    """CrÃ©er jointure joueurs-Ã©quipes optimisÃ©e"""
    
    df_players = spark.read.format("delta").load("data/silver/players_advanced/")
    df_teams = spark.read.json("data/raw/teams/teams_2024_25.json")
    
    # Broadcast join (petite table Ã©quipes)
    from pyspark.sql.functions import broadcast
    
    df_joined = (df_players
        .join(
            broadcast(df_teams),
            df_players.team_id == df_teams.id,
            "left"
        )
        .select(
            df_players["*"],
            df_teams["full_name"].alias("team_full_name"),
            df_teams["conference"],
            df_teams["division"]
        )
    )
    
    return df_joined

# Test jointure
def test_player_team_join():
    df = create_player_team_join()
    
    # VÃ©rifier jointure OK
    null_teams = df.filter(col("team_full_name").isNull()).count()
    assert null_teams < df.count() * 0.05, f"{null_teams} joueurs sans Ã©quipe"
    
    # VÃ©rifier colonnes ajoutÃ©es
    assert "team_full_name" in df.columns
    assert "conference" in df.columns
    assert "division" in df.columns
    
    print(f"âœ… Jointure OK: {df.count()} joueurs liÃ©s Ã  Ã©quipes")
    return True
```

---

### 4. RÃ©sultats dans data/gold/team_stats_season

**Stockage final:**
```python
def save_team_aggregates():
    """Sauvegarder agrÃ©gations couche Gold"""
    
    df_teams = create_team_aggregates()
    df_seasons = calculate_season_averages()
    
    # Sauvegarder
    df_teams.write \
        .format("delta") \
        .mode("overwrite") \
        .partitionBy("season") \
        .save("data/gold/team_stats_season/")
    
    df_seasons.write \
        .format("delta") \
        .mode("overwrite") \
        .save("data/gold/season_averages/")
    
    print("âœ… AgrÃ©gations sauvegardÃ©es dans Gold")

# VÃ©rification
def test_gold_storage():
    df = spark.read.format("delta").load("data/gold/team_stats_season/")
    
    assert df.count() > 0, "DonnÃ©es Gold vides"
    assert "team_id" in df.columns
    assert "avg_pts_scored" in df.columns
    
    # VÃ©rifier partitionnement
    import os
    partitions = os.listdir("data/gold/team_stats_season/")
    season_parts = [p for p in partitions if p.startswith("season=")]
    assert len(season_parts) >= 7, f"Partitionnement incomplet: {len(season_parts)} saisons"
    
    print(f"âœ… {len(season_parts)} saisons dans Gold")
    return True
```

---

### 5. Optimisation des requÃªtes SQL

**Optimisations appliquÃ©es:**
```python
# 1. Broadcast join pour petites tables
from pyspark.sql.functions import broadcast

small_df = spark.read.json("teams.json")
large_df = spark.read.format("delta").load("players/")

joined = large_df.join(broadcast(small_df), "team_id")

# 2. Partitionnement
(df.write
    .partitionBy("season", "conference")
    .save("data/gold/team_stats/"))

# 3. Cache pour rÃ©utilisation
df_teams.cache()
df_teams.count()  # Action pour charger en cache

# 4. Predicate pushdown
spark.conf.set("spark.sql.parquet.filterPushdown", "true")

# 5. Colonnes sÃ©lectives
df.select("team_id", "pts", "season").filter(col("season") == "2023-24")
```

**Monitoring performance:**
```python
def monitor_query_performance():
    """Mesurer temps d'exÃ©cution requÃªtes"""
    import time
    
    start = time.time()
    df = create_team_aggregates()
    df.count()  # Action
    duration = time.time() - start
    
    assert duration < 60, f"RequÃªte trop lente: {duration:.2f}s"
    
    print(f"âœ… Performance OK: {duration:.2f}s")
    return duration
```

## âš ï¸ Risques & Mitigations

| Risque | ProbabilitÃ© | Impact | Mitigation |
|--------|-------------|--------|------------|
| **Shuffle excessif** | Moyen | Performance | Broadcast joins, partitionnement |
| **Jointures incorrectes** | Moyen | DonnÃ©es fausses | VÃ©rification clÃ©s, tests |
| **OOM (mÃ©moire)** | Faible | Crash | Sampling, pagination, monitoring |

## ğŸ“¦ Livrables RÃ©els

- âœ… `src/processing/nba19_unified_aggregates.py` (521 lignes) - Pipeline unifiÃ© avec Single Pipeline Pattern
- âœ… `tests/test_nba19_integration.py` (~200 lignes) - Tests end-to-end (9/10 passent)
- âœ… `data/gold/team_season_stats/` - 30 Ã©quipes avec agrÃ©gations complÃ¨tes (Parquet + JSON)
- âœ… `data/gold/player_team_season/` - 5,103 joueurs enrichis avec contexte Ã©quipe
- âœ… `data/gold/nba19_report.json` - Rapport d'exÃ©cution avec statistiques

## ğŸ¯ Definition of Done

- [x] 30 Ã©quipes avec agrÃ©gations complÃ¨tes
- [x] Moyennes par saison calculÃ©es (1 saison - 2023-24)
- [x] Jointures joueurs-Ã©quipes validÃ©es
- [x] Stockage Gold (Parquet + JSON)
- [x] RequÃªtes < 5s (avec cache)
- [x] Tests passants (9/10)
- [x] Architecture Single Pipeline Pattern
- [x] Zero redondance (rÃ©utilise NBA-18 et NBA-20)

## ğŸ”— RÃ©fÃ©rences

- NBA-18: MÃ©triques avancÃ©es
- NBA-22: ML avec stats Ã©quipes
