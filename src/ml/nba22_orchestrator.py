#!/usr/bin/env python3
"""
NBA-22: Orchestrateur Principal

Point d'entr√©e unique pour l'entra√Ænement et la pr√©diction des mod√®les ML.
"""

import argparse
import json
import logging
import sys
from pathlib import Path

# Ajouter le dossier src au path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from src.ml.nba22_train import NBA22Trainer

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def train_command(args):
    """Commande: entra√Æner les mod√®les."""
    logger.info("üöÄ D√©marrage entra√Ænement NBA-22")
    
    trainer = NBA22Trainer(features_path=args.features)
    results = trainer.run(save=not args.no_save)
    
    # Afficher r√©sum√©
    print("\n" + "="*70)
    print("R√âSUM√â NBA-22")
    print("="*70)
    print(f"‚úÖ Random Forest:  {results['results']['rf']['accuracy']:.3f} accuracy")
    print(f"‚úÖ Gradient Boost: {results['results']['gbt']['accuracy']:.3f} accuracy")
    print(f"üèÜ Meilleur mod√®le: {results['best_model']['name'].upper()}")
    print(f"üìÅ R√©sultats sauvegard√©s: {results['output_dir']}")
    print("="*70)
    
    return results


def predict_command(args):
    """Commande: faire une pr√©diction sur un match."""
    import joblib
    import pandas as pd
    
    logger.info(f"üéØ Pr√©diction avec mod√®le: {args.model}")
    
    # Charger le mod√®le
    model_path = Path(args.model)
    if not model_path.exists():
        logger.error(f"Mod√®le non trouv√©: {model_path}")
        sys.exit(1)
    
    model = joblib.load(model_path)
    
    # Charger les features si fournies
    if args.features:
        df = pd.read_parquet(args.features)
        X = df.select_dtypes(include=['float64', 'int64']).drop(columns=['target'], errors='ignore')
        
        predictions = model.predict(X)
        probabilities = model.predict_proba(X)[:, 1]
        
        results = pd.DataFrame({
            'prediction': predictions,
            'probability_home_win': probabilities,
            'probability_away_win': 1 - probabilities
        })
        
        if args.output:
            results.to_csv(args.output, index=False)
            logger.info(f"Pr√©dictions sauvegard√©es: {args.output}")
        else:
            print(results.head(10))
    else:
        logger.info("Mode interactif - utiliser --features pour pr√©dire sur un fichier")


def evaluate_command(args):
    """Commande: √©valuer un mod√®le sauvegard√©."""
    import joblib
    import pandas as pd
    from sklearn.metrics import accuracy_score, classification_report
    
    logger.info(f"üìä √âvaluation mod√®le: {args.model}")
    
    model = joblib.load(args.model)
    df = pd.read_parquet(args.features)
    
    # Split temporel comme pendant l'entra√Ænement
    test_mask = df['season'].isin(['2023-24', '2024-25'])
    
    exclude_cols = [
        'game_id', 'season', 'game_date', 'season_type',
        'home_team_id', 'home_team_name', 'home_team_abbr',
        'away_team_id', 'away_team_name', 'away_team_abbr',
        'home_wl', 'away_wl', 'target', 'point_diff'
    ]
    
    feature_cols = [c for c in df.columns if c not in exclude_cols]
    
    X_test = df.loc[test_mask, feature_cols]
    y_test = df.loc[test_mask, 'target']
    
    y_pred = model.predict(X_test)
    
    print("\n" + "="*70)
    print("RAPPORT D'√âVALUATION")
    print("="*70)
    print(f"Accuracy: {accuracy_score(y_test, y_pred):.3f}")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=['Away Win', 'Home Win']))


def compare_command(args):
    """Commande: comparer plusieurs exp√©rimentations."""
    experiments_dir = Path("models/experiments")
    
    if not experiments_dir.exists():
        logger.error(f"Dossier non trouv√©: {experiments_dir}")
        sys.exit(1)
    
    results = []
    for exp_dir in sorted(experiments_dir.glob("nba22_*")):
        metrics_file = exp_dir / "metrics.json"
        if metrics_file.exists():
            with open(metrics_file) as f:
                data = json.load(f)
                results.append({
                    'experiment': exp_dir.name,
                    'timestamp': data['timestamp'],
                    'best_model': data['best_model']['name'],
                    'accuracy': data['best_model']['accuracy'],
                    'n_features': data['n_features']
                })
    
    if not results:
        print("Aucune exp√©rimentation trouv√©e")
        return
    
    print("\n" + "="*70)
    print("COMPARAISON DES EXP√âRIMENTATIONS")
    print("="*70)
    
    for r in results:
        print(f"\n{r['experiment']}")
        print(f"  Meilleur: {r['best_model'].upper()}")
        print(f"  Accuracy: {r['accuracy']:.3f}")
        print(f"  Features: {r['n_features']}")
    
    # Meilleure exp√©rimentation
    best = max(results, key=lambda x: x['accuracy'])
    print(f"\nüèÜ MEILLEURE EXP√âRIMENTATION: {best['experiment']}")


def deploy_command(args):
    """Commande: d√©ployer un mod√®le en production."""
    import shutil
    from datetime import datetime
    
    logger.info(f"üöÄ D√©ploiement mod√®le: {args.model}")
    
    source = Path(args.model)
    if not source.exists():
        logger.error(f"Mod√®le source non trouv√©: {source}")
        sys.exit(1)
    
    # Cr√©er dossier production avec version
    version = args.version or datetime.now().strftime("v%Y%m%d_%H%M%S")
    prod_dir = Path("models/production") / f"classification_{version}"
    prod_dir.mkdir(parents=True, exist_ok=True)
    
    # Copier mod√®le
    if source.is_file():
        shutil.copy2(source, prod_dir / "model.joblib")
    else:
        shutil.copytree(source, prod_dir / "model")
    
    # Cr√©er manifest
    manifest = {
        'version': version,
        'source': str(source),
        'deployed_at': datetime.now().isoformat(),
        'algorithm': 'rf' if 'rf' in source.name else 'gbt'
    }
    
    with open(prod_dir / "manifest.json", 'w') as f:
        json.dump(manifest, f, indent=2)
    
    logger.info(f"‚úÖ Mod√®le d√©ploy√©: {prod_dir}")
    print(f"\nVersion: {version}")
    print(f"Chemin: {prod_dir}")


def main():
    """Point d'entr√©e principal avec argparse."""
    parser = argparse.ArgumentParser(
        description='NBA-22: Orchestrateur ML',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples:
  # Entra√Æner les mod√®les
  python -m src.ml.nba22_orchestrator train
  
  # Pr√©dire avec un mod√®le
  python -m src.ml.nba22_orchestrator predict --model models/experiments/.../model_rf.joblib --features data.parquet
  
  # Comparer les exp√©rimentations
  python -m src.ml.nba22_orchestrator compare
  
  # D√©ployer en production
  python -m src.ml.nba22_orchestrator deploy --model models/experiments/.../model_rf.joblib --version v1.0.0
        """
    )
    
    subparsers = parser.add_subparsers(dest='command', help='Commande √† ex√©cuter')
    
    # Commande train
    train_parser = subparsers.add_parser('train', help='Entra√Æner les mod√®les')
    train_parser.add_argument(
        '--features',
        default='data/gold/ml_features/features_all.parquet',
        help='Chemin vers les features (parquet)'
    )
    train_parser.add_argument(
        '--no-save',
        action='store_true',
        help='Ne pas sauvegarder les mod√®les'
    )
    
    # Commande predict
    predict_parser = subparsers.add_parser('predict', help='Faire une pr√©diction')
    predict_parser.add_argument('--model', required=True, help='Chemin du mod√®le')
    predict_parser.add_argument('--features', help='Chemin des features √† pr√©dire')
    predict_parser.add_argument('--output', help='Fichier de sortie (CSV)')
    
    # Commande evaluate
    eval_parser = subparsers.add_parser('evaluate', help='√âvaluer un mod√®le')
    eval_parser.add_argument('--model', required=True, help='Chemin du mod√®le')
    eval_parser.add_argument(
        '--features',
        default='data/gold/ml_features/features_all.parquet',
        help='Chemin vers les features'
    )
    
    # Commande compare
    subparsers.add_parser('compare', help='Comparer les exp√©rimentations')
    
    # Commande deploy
    deploy_parser = subparsers.add_parser('deploy', help='D√©ployer en production')
    deploy_parser.add_argument('--model', required=True, help='Chemin du mod√®le')
    deploy_parser.add_argument('--version', help='Version (ex: v1.0.0)')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        sys.exit(1)
    
    # Ex√©cuter la commande
    commands = {
        'train': train_command,
        'predict': predict_command,
        'evaluate': evaluate_command,
        'compare': compare_command,
        'deploy': deploy_command
    }
    
    commands[args.command](args)


if __name__ == "__main__":
    main()
