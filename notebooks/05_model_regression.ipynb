{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA-22-2: Modèle de Régression (Score Exact)\n",
    "\n",
    "**Objectif:** Prédire le score exact des deux équipes\n",
    "\n",
    "**Challenge:** Plus difficile que la classification car variance élevée\n",
    "\n",
    "**Objectif de performance:** MAE < 10 points\n",
    "\n",
    "**Approche:** Deux modèles (un par équipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "spark = SparkSession.builder.appName(\"NBA-Regression\").getOrCreate()\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger données\n",
    "df = spark.read.parquet(\"../data/gold/ml_features\")\n",
    "\n",
    "# Vérifier les colonnes de score\n",
    "print(\"Colonnes disponibles:\")\n",
    "for col_name in df.columns:\n",
    "    if 'score' in col_name.lower():\n",
    "        print(f\"  - {col_name}\")\n",
    "\n",
    "df.select('game_id', 'score_home', 'score_away').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Features Spécifiques Régression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features de base + features avancées pour régression\n",
    "feature_cols = [\n",
    "    # Features classification\n",
    "    'win_pct_home',\n",
    "    'win_pct_away',\n",
    "    'win_pct_last_5_home',\n",
    "    'win_pct_last_5_away',\n",
    "    'avg_points_home',\n",
    "    'avg_points_away',\n",
    "    'rest_days_home',\n",
    "    'rest_days_away',\n",
    "    \n",
    "    # Features additionnelles pour régression\n",
    "    'pace_home',  # Rythme de jeu\n",
    "    'pace_away',\n",
    "    'offensive_rating_home',\n",
    "    'offensive_rating_away',\n",
    "    'defensive_rating_home',\n",
    "    'defensive_rating_away',\n",
    "    'effective_fg_pct_home',\n",
    "    'effective_fg_pct_away',\n",
    "    'turnover_rate_home',\n",
    "    'turnover_rate_away',\n",
    "]\n",
    "\n",
    "print(f\"Nombre de features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Split Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split temporel\n",
    "train_df = df.filter(df.season.isin(['2018-19', '2019-20', '2020-21', '2021-22']))\n",
    "val_df = df.filter(df.season == '2022-23')\n",
    "test_df = df.filter(df.season == '2023-24')\n",
    "\n",
    "print(f\"Train: {train_df.count()} matchs\")\n",
    "print(f\"Validation: {val_df.count()} matchs\")\n",
    "print(f\"Test: {test_df.count()} matchs\")\n",
    "\n",
    "# Stats des scores (pour baseline)\n",
    "train_df.select('score_home', 'score_away').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Baseline: Moyenne Saison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline simple: prédire la moyenne\n",
    "from pyspark.sql.functions import avg as spark_avg, abs as spark_abs\n",
    "\n",
    "mean_scores = train_df.agg(\n",
    "    spark_avg('score_home').alias('mean_home'),\n",
    "    spark_avg('score_away').alias('mean_away')\n",
    ").collect()[0]\n",
    "\n",
    "mean_home = mean_scores['mean_home']\n",
    "mean_away = mean_scores['mean_away']\n",
    "\n",
    "print(f\"Score moyen équipe à domicile: {mean_home:.1f}\")\n",
    "print(f\"Score moyen équipe à l'extérieur: {mean_away:.1f}\")\n",
    "\n",
    "# Calculer MAE baseline sur test\n",
    "baseline_df = test_df.withColumn('pred_home', F.lit(mean_home))\n",
    "baseline_df = baseline_df.withColumn('pred_away', F.lit(mean_away))\n",
    "baseline_df = baseline_df.withColumn('ae_home', spark_abs(F.col('score_home') - F.col('pred_home')))\n",
    "baseline_df = baseline_df.withColumn('ae_away', spark_abs(F.col('score_away') - F.col('pred_away')))\n",
    "\n",
    "baseline_mae = baseline_df.agg(\n",
    "    spark_avg('ae_home').alias('mae_home'),\n",
    "    spark_avg('ae_away').alias('mae_away')\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"\\nBaseline MAE Home: {baseline_mae['mae_home']:.2f} points\")\n",
    "print(f\"Baseline MAE Away: {baseline_mae['mae_away']:.2f} points\")\n",
    "print(f\"Baseline MAE Moyen: {(baseline_mae['mae_home'] + baseline_mae['mae_away'])/2:.2f} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modèles Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "# Modèle pour équipe à domicile\n",
    "rf_home = RandomForestRegressor(\n",
    "    labelCol=\"score_home\",\n",
    "    featuresCol=\"features\",\n",
    "    numTrees=200,\n",
    "    maxDepth=15,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Modèle pour équipe à l'extérieur\n",
    "rf_away = RandomForestRegressor(\n",
    "    labelCol=\"score_away\",\n",
    "    featuresCol=\"features\",\n",
    "    numTrees=200,\n",
    "    maxDepth=15,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Pipelines\n",
    "pipeline_home = Pipeline(stages=[assembler, rf_home])\n",
    "pipeline_away = Pipeline(stages=[assembler, rf_away])\n",
    "\n",
    "# Entraînement\n",
    "print(\"Entraînement modèle HOME...\")\n",
    "model_home = pipeline_home.fit(train_df)\n",
    "\n",
    "print(\"Entraînement modèle AWAY...\")\n",
    "model_away = pipeline_away.fit(train_df)\n",
    "\n",
    "print(\"✅ Modèles entraînés\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédictions\n",
    "pred_home = model_home.transform(test_df)\n",
    "pred_away = model_away.transform(test_df)\n",
    "\n",
    "# Évaluateur\n",
    "evaluator = RegressionEvaluator()\n",
    "\n",
    "# Métriques Home\n",
    "mae_home = evaluator.evaluate(\n",
    "    pred_home,\n",
    "    {evaluator.labelCol: \"score_home\", evaluator.predictionCol: \"prediction\", evaluator.metricName: \"mae\"}\n",
    ")\n",
    "rmse_home = evaluator.evaluate(\n",
    "    pred_home,\n",
    "    {evaluator.labelCol: \"score_home\", evaluator.predictionCol: \"prediction\", evaluator.metricName: \"rmse\"}\n",
    ")\n",
    "r2_home = evaluator.evaluate(\n",
    "    pred_home,\n",
    "    {evaluator.labelCol: \"score_home\", evaluator.predictionCol: \"prediction\", evaluator.metricName: \"r2\"}\n",
    ")\n",
    "\n",
    "# Métriques Away\n",
    "mae_away = evaluator.evaluate(\n",
    "    pred_away,\n",
    "    {evaluator.labelCol: \"score_away\", evaluator.predictionCol: \"prediction\", evaluator.metricName: \"mae\"}\n",
    ")\n",
    "rmse_away = evaluator.evaluate(\n",
    "    pred_away,\n",
    "    {evaluator.labelCol: \"score_away\", evaluator.predictionCol: \"prediction\", evaluator.metricName: \"rmse\"}\n",
    ")\n",
    "r2_away = evaluator.evaluate(\n",
    "    pred_away,\n",
    "    {evaluator.labelCol: \"score_away\", evaluator.predictionCol: \"prediction\", evaluator.metricName: \"r2\"}\n",
    ")\n",
    "\n",
    "# Moyennes\n",
    "mae_avg = (mae_home + mae_away) / 2\n",
    "rmse_avg = (rmse_home + rmse_away) / 2\n",
    "\n",
    "print(\"\\n=== RÉSULTATS ===\")\n",
    "print(f\"Home - MAE: {mae_home:.2f}, RMSE: {rmse_home:.2f}, R²: {r2_home:.3f}\")\n",
    "print(f\"Away - MAE: {mae_away:.2f}, RMSE: {rmse_away:.2f}, R²: {r2_away:.3f}\")\n",
    "print(f\"\\nMoyenne - MAE: {mae_avg:.2f}, RMSE: {rmse_avg:.2f}\")\n",
    "print(f\"\\nAmélioration vs Baseline: {(baseline_mae['mae_home'] + baseline_mae['mae_away'])/2 - mae_avg:.2f} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualisation des Erreurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir en pandas pour visualisation\n",
    "pred_pd = pred_home.select('score_home', 'prediction').toPandas()\n",
    "pred_pd.columns = ['actual', 'predicted']\n",
    "pred_pd['error'] = pred_pd['actual'] - pred_pd['predicted']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Scatter plot\n",
    "axes[0].scatter(pred_pd['actual'], pred_pd['predicted'], alpha=0.5)\n",
    "axes[0].plot([80, 140], [80, 140], 'r--', lw=2)\n",
    "axes[0].set_xlabel(\"Score Réel\")\n",
    "axes[0].set_ylabel(\"Score Prédit\")\n",
    "axes[0].set_title(\"Score Home: Réel vs Prédit\")\n",
    "\n",
    "# Distribution des erreurs\n",
    "axes[1].hist(pred_pd['error'], bins=30, edgecolor='black')\n",
    "axes[1].axvline(x=0, color='r', linestyle='--')\n",
    "axes[1].set_xlabel(\"Erreur (points)\")\n",
    "axes[1].set_ylabel(\"Fréquence\")\n",
    "axes[1].set_title(\"Distribution des Erreurs - Home Team\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Erreur moyenne: {pred_pd['error'].mean():.2f} points\")\n",
    "print(f\"Erreur std: {pred_pd['error'].std():.2f} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analyse des Erreurs Élevées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifier les matchs avec erreur > 15 points\n",
    "high_errors = pred_pd[abs(pred_pd['error']) > 15]\n",
    "print(f\"Matchs avec erreur > 15 pts: {len(high_errors)} ({len(high_errors)/len(pred_pd)*100:.1f}%)\")\n",
    "\n",
    "# Caractéristiques communes?\n",
    "# TODO: Analyser ces matchs en détail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Sauvegarde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder les modèles\n",
    "model_home.save(\"../models/regression_home\")\n",
    "model_away.save(\"../models/regression_away\")\n",
    "\n",
    "# Sauvegarder les métriques\n",
    "import json\n",
    "metrics = {\n",
    "    'mae_home': mae_home,\n",
    "    'mae_away': mae_away,\n",
    "    'mae_avg': mae_avg,\n",
    "    'rmse_home': rmse_home,\n",
    "    'rmse_away': rmse_away,\n",
    "    'r2_home': r2_home,\n",
    "    'r2_away': r2_away,\n",
    "    'baseline_mae': (baseline_mae['mae_home'] + baseline_mae['mae_away'])/2\n",
    "}\n",
    "\n",
    "with open(\"../models/regression_metrics.json\", 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(\"✅ Modèles et métriques sauvegardés\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résumé\n",
    "\n",
    "**Résultats:**\n",
    "- Baseline MAE: XX.XX points\n",
    "- **Modèle MAE: XX.XX points** ← Objectif < 10 points\n",
    "- Amélioration: XX.XX points\n",
    "\n",
    "**Conclusion:** [À remplir]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
