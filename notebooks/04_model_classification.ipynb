{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA-22-1: Modèle de Classification (Gagnant/Perdant)\n",
    "\n",
    "**Objectif:** Prédire le gagnant d'un match NBA avec PySpark ML\n",
    "\n",
    "**Target:** Winner (0 = perdant, 1 = gagnant)  \n",
    "**Objectif de performance:** Accuracy > 65%\n",
    "\n",
    "**Algorithme:** Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup et Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NBA-Classification\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le dataset ML créé par NBA-21\n",
    "df = spark.read.parquet(\"../data/gold/ml_features\")\n",
    "\n",
    "print(f\"Shape: ({df.count()}, {len(df.columns)})\")\n",
    "print(f\"Colonnes: {df.columns}\")\n",
    "\n",
    "# Aperçu\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyse Exploratoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution de la target\n",
    "df.groupBy(\"target\").count().show()\n",
    "\n",
    "# Balance des classes\n",
    "target_dist = df.groupBy(\"target\").count().toPandas()\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(data=target_dist, x=\"target\", y=\"count\")\n",
    "plt.title(\"Distribution: Gagnant vs Perdant\")\n",
    "plt.xlabel(\"Target (0=Perdant, 1=Gagnant)\")\n",
    "plt.ylabel(\"Nombre de matchs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Préparation des Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les features à utiliser\n",
    "feature_cols = [\n",
    "    'win_pct_home',\n",
    "    'win_pct_away',\n",
    "    'win_pct_last_5_home',\n",
    "    'win_pct_last_5_away',\n",
    "    'avg_points_home',\n",
    "    'avg_points_away',\n",
    "    'rest_days_home',\n",
    "    'rest_days_away',\n",
    "    'is_back_to_back_home',\n",
    "    'is_back_to_back_away',\n",
    "    # Ajouter d'autres features ici\n",
    "]\n",
    "\n",
    "print(f\"Nombre de features: {len(feature_cols)}\")\n",
    "print(\"Features:\", feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Split Train/Test (Temporel !)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Split temporel pour éviter la fuite de données\n",
    "# Train: saisons 2018-2022\n",
    "# Validation: saison 2022-2023\n",
    "# Test: saison 2023-2024\n",
    "\n",
    "train_df = df.filter(df.season.isin(['2018-19', '2019-20', '2020-21', '2021-22']))\n",
    "val_df = df.filter(df.season == '2022-23')\n",
    "test_df = df.filter(df.season == '2023-24')\n",
    "\n",
    "print(f\"Train: {train_df.count()} matchs\")\n",
    "print(f\"Validation: {val_df.count()} matchs\")\n",
    "print(f\"Test: {test_df.count()} matchs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modèle Baseline: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"scaled_features\"\n",
    ")\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"target\",\n",
    "    featuresCol=\"scaled_features\",\n",
    "    numTrees=100,\n",
    "    maxDepth=10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, scaler, rf])\n",
    "\n",
    "# Entraînement\n",
    "model = pipeline.fit(train_df)\n",
    "print(\"✅ Modèle entraîné\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédictions\n",
    "train_pred = model.transform(train_df)\n",
    "val_pred = model.transform(val_df)\n",
    "test_pred = model.transform(test_df)\n",
    "\n",
    "# Évaluateurs\n",
    "evaluators = {\n",
    "    'accuracy': MulticlassClassificationEvaluator(\n",
    "        labelCol=\"target\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    "    ),\n",
    "    'precision': MulticlassClassificationEvaluator(\n",
    "        labelCol=\"target\", predictionCol=\"prediction\", metricName=\"weightedPrecision\"\n",
    "    ),\n",
    "    'recall': MulticlassClassificationEvaluator(\n",
    "        labelCol=\"target\", predictionCol=\"prediction\", metricName=\"weightedRecall\"\n",
    "    ),\n",
    "    'f1': MulticlassClassificationEvaluator(\n",
    "        labelCol=\"target\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    "    ),\n",
    "    'auc': BinaryClassificationEvaluator(\n",
    "        labelCol=\"target\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# Calcul des métriques\n",
    "results = {}\n",
    "for split_name, split_df in [('Train', train_pred), ('Validation', val_pred), ('Test', test_pred)]:\n",
    "    results[split_name] = {}\n",
    "    for metric_name, evaluator in evaluators.items():\n",
    "        results[split_name][metric_name] = evaluator.evaluate(split_df)\n",
    "\n",
    "# Affichage\n",
    "results_df = pd.DataFrame(results).round(4)\n",
    "print(\"\\nMétriques:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire l'importance des features\n",
    "rf_model = model.stages[-1]\n",
    "importances = rf_model.featureImportances.toArray()\n",
    "\n",
    "# DataFrame pour visualisation\n",
    "feat_imp = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=feat_imp.head(15), x='importance', y='feature')\n",
    "plt.title('Top 15 Features - Random Forest')\n",
    "plt.xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 features les plus importantes:\")\n",
    "print(feat_imp.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Optimisation avec Grid Search (Optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Décommenter pour faire du grid search\n",
    "# param_grid = ParamGridBuilder() \\\n",
    "#     .addGrid(rf.numTrees, [50, 100, 200]) \\\n",
    "#     .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
    "#     .addGrid(rf.maxBins, [16, 32]) \\\n",
    "#     .build()\n",
    "# \n",
    "# tvs = TrainValidationSplit(\n",
    "#     estimator=pipeline,\n",
    "#     estimatorParamMaps=param_grid,\n",
    "#     evaluator=evaluators['accuracy'],\n",
    "#     trainRatio=0.8,\n",
    "#     seed=42\n",
    "# )\n",
    "# \n",
    "# best_model = tvs.fit(train_df)\n",
    "# print(\"✅ Grid search terminé\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Sauvegarde du Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le modèle\n",
    "model_path = \"../models/classification_baseline\"\n",
    "model.save(model_path)\n",
    "print(f\"✅ Modèle sauvegardé: {model_path}\")\n",
    "\n",
    "# Sauvegarder les métriques\n",
    "import json\n",
    "metrics_path = \"../models/classification_metrics.json\"\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"✅ Métriques sauvegardées: {metrics_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résumé\n",
    "\n",
    "**Résultats:**\n",
    "- Accuracy Train: X.XXXX\n",
    "- Accuracy Validation: X.XXXX\n",
    "- **Accuracy Test: X.XXXX** ← Objectif > 65%\n",
    "\n",
    "**Features importantes:**\n",
    "1. Feature A (XX.X%)\n",
    "2. Feature B (XX.X%)\n",
    "3. Feature C (XX.X%)\n",
    "\n",
    "**Conclusion:** [À remplir après exécution]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
