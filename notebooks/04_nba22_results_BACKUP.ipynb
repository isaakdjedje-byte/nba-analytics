{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA-22: RÃ©sultats des ModÃ¨les ML\n",
    "\n",
    "Analyse des performances des modÃ¨les Random Forest et Gradient Boosting pour la prÃ©diction des matchs NBA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m-------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, roc_curve, auc\n\u001b[32m      9\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mmatplotlib\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33minline\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     10\u001b[39m sns.set_style(\u001b[33m\"\u001b[39m\u001b[33mwhitegrid\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement des DonnÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les features\n",
    "df = pd.read_parquet(\"../data/gold/ml_features/features_all.parquet\")\n",
    "\n",
    "print(f\"Dataset: {len(df)} matchs\")\n",
    "print(f\"Saisons: {sorted(df['season'].unique())}\")\n",
    "print(f\"Features: {len(df.columns)} colonnes\")\n",
    "print(f\"\\nDistribution target:\")\n",
    "print(df['target'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement des RÃ©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trouver la derniÃ¨re expÃ©rimentation\n",
    "exp_dir = sorted(Path(\"../models/experiments\").glob(\"nba22_*\"))[-1]\n",
    "metrics_file = exp_dir / \"metrics.json\"\n",
    "\n",
    "with open(metrics_file) as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "print(f\"ExpÃ©rimentation: {exp_dir.name}\")\n",
    "print(f\"Timestamp: {results['timestamp']}\")\n",
    "print(f\"Train size: {results['train_size']}\")\n",
    "print(f\"Test size: {results['test_size']}\")\n",
    "print(f\"N features: {results['n_features']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparaison des ModÃ¨les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er DataFrame de comparaison\n",
    "comparison_data = []\n",
    "for model_name, metrics in results['models'].items():\n",
    "    comparison_data.append({\n",
    "        'ModÃ¨le': metrics['model_name'],\n",
    "        'Accuracy': metrics['accuracy'],\n",
    "        'Precision': metrics['precision'],\n",
    "        'Recall': metrics['recall'],\n",
    "        'F1-Score': metrics['f1'],\n",
    "        'AUC': metrics['auc']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.set_index('ModÃ¨le')\n",
    "\n",
    "display(comparison_df.style.background_gradient(cmap='YlGn', axis=0).format('{:.3f}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Barplot des mÃ©triques\n",
    "comparison_df.plot(kind='bar', ax=axes[0])\n",
    "axes[0].set_title('Comparaison des MÃ©triques')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].axhline(y=0.60, color='r', linestyle='--', label='Objectif (60%)')\n",
    "\n",
    "# Radar chart simplifiÃ©\n",
    "metrics_cols = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']\n",
    "x = np.arange(len(metrics_cols))\n",
    "width = 0.35\n",
    "\n",
    "rf_values = comparison_df.loc['Random Forest', metrics_cols].values\n",
    "gbt_values = comparison_df.loc['Gradient Boosting', metrics_cols].values\n",
    "\n",
    "axes[1].bar(x - width/2, rf_values, width, label='Random Forest', alpha=0.8)\n",
    "axes[1].bar(x + width/2, gbt_values, width, label='Gradient Boosting', alpha=0.8)\n",
    "axes[1].set_xlabel('MÃ©triques')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_title('DÃ©tail par MÃ©trique')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(metrics_cols, rotation=45)\n",
    "axes[1].legend()\n",
    "axes[1].axhline(y=0.60, color='r', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 15 features pour chaque modÃ¨le\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "for idx, (model_name, metrics) in enumerate(results['models'].items()):\n",
    "    importance = metrics['feature_importance']\n",
    "    top_features = dict(list(importance.items())[:15])\n",
    "    \n",
    "    df_imp = pd.DataFrame(\n",
    "        list(top_features.items()),\n",
    "        columns=['Feature', 'Importance']\n",
    "    )\n",
    "    \n",
    "    df_imp.plot(x='Feature', y='Importance', kind='barh', ax=axes[idx])\n",
    "    axes[idx].set_title(f\"Top 15 Features - {metrics['model_name']}\")\n",
    "    axes[idx].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparer feature importance entre modÃ¨les\n",
    "rf_imp = results['models']['rf']['feature_importance']\n",
    "gbt_imp = results['models']['gbt']['feature_importance']\n",
    "\n",
    "# Top 10 features communes\n",
    "top_rf = set(list(rf_imp.keys())[:10])\n",
    "top_gbt = set(list(gbt_imp.keys())[:10])\n",
    "common = top_rf.intersection(top_gbt)\n",
    "\n",
    "print(f\"\\nTop 10 RF: {len(top_rf)} features\")\n",
    "print(f\"Top 10 GBT: {len(top_gbt)} features\")\n",
    "print(f\"Communes: {len(common)} features\")\n",
    "print(f\"\\nFeatures communes:\")\n",
    "for f in common:\n",
    "    print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyse des PrÃ©dictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le meilleur modÃ¨le\n",
    "import joblib\n",
    "\n",
    "best_model_name = results['best_model']['name']\n",
    "model_path = exp_dir / f\"model_{best_model_name}.joblib\"\n",
    "model = joblib.load(model_path)\n",
    "\n",
    "print(f\"Meilleur modÃ¨le: {best_model_name.upper()}\")\n",
    "print(f\"Accuracy: {results['best_model']['accuracy']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PrÃ©parer donnÃ©es test\n",
    "test_mask = df['season'].isin(['2023-24', '2024-25'])\n",
    "\n",
    "exclude_cols = [\n",
    "    'game_id', 'season', 'game_date', 'season_type',\n",
    "    'home_team_id', 'home_team_name', 'home_team_abbr',\n",
    "    'away_team_id', 'away_team_name', 'away_team_abbr',\n",
    "    'home_wl', 'away_wl', 'target', 'point_diff'\n",
    "]\n",
    "\n",
    "feature_cols = [c for c in df.columns if c not in exclude_cols]\n",
    "\n",
    "X_test = df.loc[test_mask, feature_cols]\n",
    "y_test = df.loc[test_mask, 'target']\n",
    "\n",
    "# PrÃ©dictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Confusion Matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Away Win', 'Home Win'],\n",
    "            yticklabels=['Away Win', 'Home Win'])\n",
    "axes[0].set_title('Matrice de Confusion')\n",
    "axes[0].set_ylabel('RÃ©el')\n",
    "axes[0].set_xlabel('PrÃ©dit')\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "axes[1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "axes[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "axes[1].set_xlim([0.0, 1.0])\n",
    "axes[1].set_ylim([0.0, 1.05])\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].set_title('Courbe ROC')\n",
    "axes[1].legend(loc=\"lower right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyse par Saison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance par saison (test set)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "season_results = []\n",
    "for season in ['2023-24', '2024-25']:\n",
    "    season_mask = (df['season'] == season) & test_mask\n",
    "    if season_mask.sum() > 0:\n",
    "        X_season = df.loc[season_mask, feature_cols]\n",
    "        y_season = df.loc[season_mask, 'target']\n",
    "        y_pred_season = model.predict(X_season)\n",
    "        \n",
    "        acc = accuracy_score(y_season, y_pred_season)\n",
    "        season_results.append({\n",
    "            'Saison': season,\n",
    "            'Matchs': len(y_season),\n",
    "            'Accuracy': acc,\n",
    "            'Home Win %': y_season.mean()\n",
    "        })\n",
    "\n",
    "season_df = pd.DataFrame(season_results)\n",
    "display(season_df)\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(season_df['Saison'], season_df['Accuracy'], alpha=0.7)\n",
    "plt.axhline(y=0.60, color='r', linestyle='--', label='Objectif (60%)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Performance par Saison (Test Set)')\n",
    "plt.legend()\n",
    "plt.ylim(0.5, 0.7)\n",
    "for i, row in season_df.iterrows():\n",
    "    plt.text(i, row['Accuracy'] + 0.01, f\"{row['Accuracy']:.3f}\", \n",
    "             ha='center', fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyse des Erreurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des erreurs\n",
    "errors = df.loc[test_mask].copy()\n",
    "errors['prediction'] = y_pred\n",
    "errors['probability'] = y_proba\n",
    "errors['error'] = errors['target'] != errors['prediction']\n",
    "\n",
    "print(f\"Total erreurs: {errors['error'].sum()} / {len(errors)} ({errors['error'].mean():.1%})\")\n",
    "\n",
    "# Erreurs par type\n",
    "print(\"\\nErreurs par type:\")\n",
    "print(f\"  False Positives (prÃ©dit home, rÃ©el away): {((errors['prediction']==1) & (errors['target']==0)).sum()}\")\n",
    "print(f\"  False Negatives (prÃ©dit away, rÃ©el home): {((errors['prediction']==0) & (errors['target']==1)).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution des probabilitÃ©s\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Toutes les prÃ©dictions\n",
    "axes[0].hist(errors['probability'], bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0].set_xlabel('ProbabilitÃ© Home Win')\n",
    "axes[0].set_ylabel('Nombre de matchs')\n",
    "axes[0].set_title('Distribution des ProbabilitÃ©s PrÃ©dites')\n",
    "axes[0].axvline(x=0.5, color='r', linestyle='--', label='Seuil dÃ©cision')\n",
    "axes[0].legend()\n",
    "\n",
    "# Par rÃ©sultat rÃ©el\n",
    "errors[errors['target']==1]['probability'].hist(bins=30, alpha=0.5, label='Home Win (rÃ©el)', ax=axes[1])\n",
    "errors[errors['target']==0]['probability'].hist(bins=30, alpha=0.5, label='Away Win (rÃ©el)', ax=axes[1])\n",
    "axes[1].set_xlabel('ProbabilitÃ© Home Win')\n",
    "axes[1].set_ylabel('Nombre de matchs')\n",
    "axes[1].set_title('Distribution par RÃ©sultat RÃ©el')\n",
    "axes[1].axvline(x=0.5, color='r', linestyle='--')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. RÃ©sumÃ© et Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RÃ©sumÃ© final\n",
    "print(\"=\"*70)\n",
    "print(\"RÃ‰SUMÃ‰ NBA-22\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nExpÃ©rimentation: {exp_dir.name}\")\n",
    "print(f\"ModÃ¨les testÃ©s: Random Forest, Gradient Boosting\")\n",
    "print(f\"\\nğŸ† Meilleur modÃ¨le: {results['best_model']['name'].upper()}\")\n",
    "print(f\"   Accuracy: {results['best_model']['accuracy']:.3f}\")\n",
    "\n",
    "if results['best_model']['accuracy'] > 0.60:\n",
    "    print(\"   âœ… Objectif atteint (> 60%)\")\n",
    "else:\n",
    "    print(\"   âŒ Objectif non atteint (< 60%)\")\n",
    "\n",
    "print(\"\\nTop 5 features importantes:\")\n",
    "best_model_key = results['best_model']['name']\n",
    "top5 = list(results['models'][best_model_key]['feature_importance'].keys())[:5]\n",
    "for i, feat in enumerate(top5, 1):\n",
    "    print(f\"  {i}. {feat}\")\n",
    "\n",
    "print(\"\\n=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prochaines Ã‰tapes\n",
    "\n",
    "- [ ] **NBA-22-2**: RÃ©gression pour prÃ©dire le score exact\n",
    "- [ ] **NBA-22-3**: Clustering des profils de joueurs\n",
    "- [ ] **NBA-23**: DÃ©tection des joueurs en progression\n",
    "- [ ] **NBA-24**: DÃ©tection des joueurs en progression\n",
    "- [ ] **NBA-25**: Pipeline ML automatisÃ©"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
