{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Analyse du Backtest NBA 2025-26\n",
    "\n",
    "Ce notebook analyse les r√©sultats du backtesting r√©aliste sans data leakage.\n",
    "\n",
    "**M√©thodologie** :\n",
    "- Simulation jour par jour\n",
    "- Recalcul des features sans data leakage\n",
    "- Comparaison avec r√©sultats r√©els\n",
    "- M√©triques compl√®tes (accuracy, precision, recall, AUC, Brier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration des plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trouver le fichier de backtest le plus r√©cent\n",
    "predictions_dir = Path('../predictions')\n",
    "backtest_files = list(predictions_dir.glob('backtest_2025-26_*.json'))\n",
    "\n",
    "if not backtest_files:\n",
    "    print(\"‚ùå Aucun fichier de backtest trouv√©. Lancez d'abord :\")\n",
    "    print(\"   python scripts/run_backtest.py\")\n",
    "else:\n",
    "    # Charger le fichier le plus r√©cent\n",
    "    latest_file = max(backtest_files, key=lambda p: p.stat().st_mtime)\n",
    "    print(f\"üìÅ Chargement: {latest_file.name}\")\n",
    "    \n",
    "    with open(latest_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    print(f\"‚úì Donn√©es charg√©es: {len(results['predictions'])} pr√©dictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. R√©sum√© des performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©triques globales\n",
    "summary = results['summary']\n",
    "\n",
    "print(\"üìà M√©triques Globales\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total matchs analys√©s: {summary['total_games']}\")\n",
    "print(f\"Coverage: {summary['coverage']:.1%}\")\n",
    "print(f\"\")\n",
    "print(f\"Accuracy:  {summary['accuracy']:.2%}\")\n",
    "print(f\"Precision: {summary['precision']:.2%}\")\n",
    "print(f\"Recall:    {summary['recall']:.2%}\")\n",
    "print(f\"F1-Score:  {summary['f1']:.2%}\")\n",
    "print(f\"\")\n",
    "print(f\"AUC:         {summary['auc']:.4f}\")\n",
    "print(f\"Brier Score: {summary['brier_score']:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. √âvolution de l'Accuracy dans le temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er DataFrame des pr√©dictions\n",
    "df = pd.DataFrame(results['predictions'])\n",
    "df['game_date'] = pd.to_datetime(df['game_date'])\n",
    "df['cumulative_accuracy'] = df['is_correct'].expanding().mean()\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "ax.plot(df['game_date'], df['cumulative_accuracy'], linewidth=2, label='Accuracy cumul√©e')\n",
    "ax.axhline(y=summary['accuracy'], color='r', linestyle='--', \n",
    "           label=f\"Accuracy finale: {summary['accuracy']:.2%}\")\n",
    "ax.axhline(y=0.5, color='gray', linestyle=':', alpha=0.5, label='Random (50%)')\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Accuracy cumul√©e')\n",
    "ax.set_title('√âvolution de l\\'Accuracy au fil de la saison')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance par Mois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©parer donn√©es par mois\n",
    "by_month = pd.DataFrame.from_dict(results['by_month'], orient='index')\n",
    "by_month.index = pd.to_datetime(by_month.index)\n",
    "by_month = by_month.sort_index()\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Accuracy par mois\n",
    "ax1.bar(range(len(by_month)), by_month['accuracy'], color='steelblue', alpha=0.7)\n",
    "ax1.axhline(y=summary['accuracy'], color='r', linestyle='--', \n",
    "            label=f\"Moyenne: {summary['accuracy']:.2%}\")\n",
    "ax1.set_xticks(range(len(by_month)))\n",
    "ax1.set_xticklabels([d.strftime('%Y-%m') for d in by_month.index], rotation=45)\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Accuracy par Mois')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Nombre de matchs par mois\n",
    "ax2.bar(range(len(by_month)), by_month['games'], color='lightcoral', alpha=0.7)\n",
    "ax2.set_xticks(range(len(by_month)))\n",
    "ax2.set_xticklabels([d.strftime('%Y-%m') for d in by_month.index], rotation=45)\n",
    "ax2.set_ylabel('Nombre de matchs')\n",
    "ax2.set_xlabel('Mois')\n",
    "ax2.set_title('Volume de matchs par Mois')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance par Niveau de Confiance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©parer donn√©es par confiance\n",
    "by_conf = pd.DataFrame.from_dict(results['by_confidence'], orient='index')\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy par niveau\n",
    "colors = ['#2ecc71', '#f39c12', '#e74c3c']\n",
    "ax1.bar(by_conf.index, by_conf['accuracy'], color=colors, alpha=0.8)\n",
    "ax1.axhline(y=summary['accuracy'], color='black', linestyle='--', \n",
    "            label=f\"Global: {summary['accuracy']:.2%}\")\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Accuracy par Niveau de Confiance')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Distribution des matchs\n",
    "ax2.pie(by_conf['games'], labels=by_conf.index, autopct='%1.1f%%', \n",
    "        colors=colors, startangle=90)\n",
    "ax2.set_title('Distribution des Pr√©dictions par Confiance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tableau d√©taill√©\n",
    "print(\"\\nüìä D√©tail par Niveau de Confiance\")\n",
    "print(\"=\"*60)\n",
    "for level, stats in by_conf.iterrows():\n",
    "    print(f\"{level:8s}: {stats['accuracy']:.2%} ({stats['correct']:.0f}/{stats['games']:.0f})\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance par √âquipe (Top 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©parer donn√©es par √©quipe (min 10 pr√©dictions)\n",
    "by_team = pd.DataFrame.from_dict(results['by_team'], orient='index')\n",
    "by_team = by_team[by_team['predicted'] >= 10]  # Filtre\n",
    "by_team = by_team.sort_values('accuracy', ascending=False)\n",
    "\n",
    "# Top 10\n",
    "top_10 = by_team.head(10)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "y_pos = range(len(top_10))\n",
    "colors = plt.cm.RdYlGn(top_10['accuracy'])\n",
    "\n",
    "ax.barh(y_pos, top_10['accuracy'], color=colors, alpha=0.8)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(top_10.index, fontsize=9)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Accuracy')\n",
    "ax.set_title('Top 10 √âquipes - Accuracy de Pr√©diction\\n(min. 10 pr√©dictions)')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Ajouter les valeurs\n",
    "for i, (idx, row) in enumerate(top_10.iterrows()):\n",
    "    ax.text(row['accuracy'] + 0.01, i, \n",
    "            f\"{row['accuracy']:.1%} ({row['correct']:.0f}/{row['predicted']:.0f})\",\n",
    "            va='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Matrice de Confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculer matrice\n",
    "y_true = [1 if p['actual_winner'] == 'HOME' else 0 for p in results['predictions']]\n",
    "y_pred = [1 if p['predicted_winner'] == 'Home Win' else 0 for p in results['predictions']]\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "            xticklabels=['Away Win (Pred)', 'Home Win (Pred)'],\n",
    "            yticklabels=['Away Win (Real)', 'Home Win (Real)'])\n",
    "\n",
    "ax.set_ylabel('R√©sultat R√©el')\n",
    "ax.set_xlabel('Pr√©diction')\n",
    "ax.set_title('Matrice de Confusion')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculer les pourcentages\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "total = tn + fp + fn + tp\n",
    "\n",
    "print(f\"\\nüìä Matrice de Confusion (%)\")\n",
    "print(f\"Vrais N√©gatifs (Away correct):  {tn/total:.1%}\")\n",
    "print(f\"Faux Positifs (Home pr√©dit, Away r√©el): {fp/total:.1%}\")\n",
    "print(f\"Faux N√©gatifs (Away pr√©dit, Home r√©el): {fn/total:.1%}\")\n",
    "print(f\"Vrais Positifs (Home correct):  {tp/total:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Distribution des Probabilit√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©parer pr√©dictions correctes et incorrectes\n",
    "correct = df[df['is_correct'] == True]['proba_home_win']\n",
    "incorrect = df[df['is_correct'] == False]['proba_home_win']\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogramme\n",
    "ax1.hist(correct, bins=30, alpha=0.6, label='Correct', color='green', density=True)\n",
    "ax1.hist(incorrect, bins=30, alpha=0.6, label='Incorrect', color='red', density=True)\n",
    "ax1.axvline(x=0.5, color='black', linestyle='--', label='Seuil (50%)')\n",
    "ax1.set_xlabel('Probabilit√© Home Win')\n",
    "ax1.set_ylabel('Densit√©')\n",
    "ax1.set_title('Distribution des Probabilit√©s')\n",
    "ax1.legend()\n",
    "\n",
    "# Box plot\n",
    "data_to_plot = [correct, incorrect]\n",
    "bp = ax2.boxplot(data_to_plot, labels=['Correct', 'Incorrect'], patch_artist=True)\n",
    "bp['boxes'][0].set_facecolor('green')\n",
    "bp['boxes'][0].set_alpha(0.6)\n",
    "bp['boxes'][1].set_facecolor('red')\n",
    "bp['boxes'][1].set_alpha(0.6)\n",
    "ax2.axhline(y=0.5, color='black', linestyle='--', alpha=0.5)\n",
    "ax2.set_ylabel('Probabilit√© Home Win')\n",
    "ax2.set_title('Distribution par R√©sultat')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Statistiques des Probabilit√©s\")\n",
    "print(f\"Correct - Moyenne: {correct.mean():.3f}, √âcart-type: {correct.std():.3f}\")\n",
    "print(f\"Incorrect - Moyenne: {incorrect.mean():.3f}, √âcart-type: {incorrect.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Calibration des Probabilit√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Calculer courbe de calibration\n",
    "y_true = [1 if p['actual_winner'] == 'HOME' else 0 for p in results['predictions']]\n",
    "y_proba = [p['proba_home_win'] for p in results['predictions']]\n",
    "\n",
    "prob_true, prob_pred = calibration_curve(y_true, y_proba, n_bins=10)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "ax.plot(prob_pred, prob_true, 's-', label='Mod√®le', markersize=8)\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Parfaitement calibr√©')\n",
    "ax.set_xlabel('Probabilit√© pr√©dite')\n",
    "ax.set_ylabel('Fraction de positifs')\n",
    "ax.set_title('Courbe de Calibration')\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Brier Score: {summary['brier_score']:.4f}\")\n",
    "print(\"(Plus proche de 0 = mieux calibr√©)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export des R√©sultats\n",
    "\n",
    "Les r√©sultats sont d√©j√† sauvegard√©s dans les fichiers :\n",
    "- JSON complet avec toutes les m√©triques\n",
    "- CSV avec les pr√©dictions d√©taill√©es\n",
    "- R√©sum√© textuel\n",
    "\n",
    "Vous pouvez les retrouver dans : `../predictions/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les fichiers g√©n√©r√©s\n",
    "predictions_dir = Path('../predictions')\n",
    "backtest_files = sorted(predictions_dir.glob('backtest_*.json'))[-3:]  # 3 derniers\n",
    "\n",
    "print(\"üìÅ Fichiers de backtest disponibles :\")\n",
    "for f in backtest_files:\n",
    "    size_mb = f.stat().st_size / (1024 * 1024)\n",
    "    mtime = datetime.fromtimestamp(f.stat().st_mtime)\n",
    "    print(f\"  {f.name:<40} {size_mb:>6.2f} MB  {mtime.strftime('%Y-%m-%d %H:%M')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Conclusion\n",
    "\n",
    "Ce backtest r√©aliste (sans data leakage) donne une estimation fiable des performances du mod√®le en production.\n",
    "\n",
    "**Interpr√©tation des r√©sultats :**\n",
    "- Accuracy > 70% : Bon mod√®le\n",
    "- Accuracy > 75% : Excellent mod√®le\n",
    "- Brier Score < 0.2 : Bonne calibration\n",
    "- High Confidence > 80% : Tr√®s fiable quand confiant\n",
    "\n",
    "**Recommandations :**\n",
    "- Se fier principalement aux pr√©dictions HIGH confidence\n",
    "- √âviter de parier sur LOW confidence\n",
    "- Surveiller les √©quipes avec faible accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
