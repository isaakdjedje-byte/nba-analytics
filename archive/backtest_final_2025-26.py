#!/usr/bin/env python3
"""
Backtest Final 2025-26

√âvalue le mod√®le unifi√© sur la saison 2025-26 avec les vraies features V3.
Compare avec l'ancien syst√®me (fallback).

Usage:
    python scripts/backtest_final_2025-26.py
"""

import sys
import json
import logging
from pathlib import Path
from datetime import datetime

import pandas as pd
import numpy as np
import joblib
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def backtest_2025_26():
    """Backtest sur la saison 2025-26."""
    logger.info("="*70)
    logger.info("BACKTEST FINAL - SAISON 2025-26")
    logger.info("="*70)
    
    # === √âTAPE 1: Charger le mod√®le ===
    logger.info("\n[1/4] Chargement du mod√®le unifi√©...")
    model_path = Path("models/unified/xgb_unified_latest.joblib")
    if not model_path.exists():
        logger.error(f"‚ùå Mod√®le non trouv√©: {model_path}")
        return
    
    model = joblib.load(model_path)
    logger.info(f"‚úì Mod√®le charg√©: {model_path}")
    
    # === √âTAPE 2: Charger les features ===
    logger.info("\n[2/4] Chargement des features 2025-26...")
    features_path = Path("data/gold/ml_features/features_2025-26_v3.parquet")
    if not features_path.exists():
        logger.error(f"‚ùå Features non trouv√©es: {features_path}")
        return
    
    df = pd.read_parquet(features_path)
    logger.info(f"‚úì {len(df)} matchs charg√©s")
    logger.info(f"‚úì {len(df.columns)} features disponibles")
    
    # === √âTAPE 3: Charger les features s√©lectionn√©es ===
    logger.info("\n[3/4] S√©lection des features...")
    selected_features_file = Path("models/optimized/selected_features.json")
    if selected_features_file.exists():
        with open(selected_features_file, 'r') as f:
            selected_features = json.load(f)['features']
    else:
        # Fallback: utiliser toutes les features sauf metadata
        exclude_cols = ['game_id', 'season', 'game_date', 'season_type', 
                       'home_team_id', 'away_team_id', 'target']
        selected_features = [c for c in df.columns if c not in exclude_cols]
    
    logger.info(f"‚úì {len(selected_features)} features s√©lectionn√©es")
    
    # === √âTAPE 4: Pr√©parer les donn√©es ===
    X = df[selected_features]
    y_true = df['target']
    
    # Filtrer uniquement les matchs jou√©s (avec target)
    mask_played = y_true.notna()
    X_played = X[mask_played]
    y_played = y_true[mask_played]
    
    logger.info(f"‚úì {len(X_played)} matchs avec r√©sultats connus")
    
    # === √âTAPE 5: Pr√©dictions ===
    logger.info("\n[4/4] Pr√©dictions...")
    y_pred = model.predict(X_played)
    y_proba = model.predict_proba(X_played)[:, 1]
    
    # === √âTAPE 6: Calculer m√©triques ===
    accuracy = accuracy_score(y_played, y_pred)
    precision = precision_score(y_played, y_pred, zero_division=0)
    recall = recall_score(y_played, y_pred, zero_division=0)
    f1 = f1_score(y_played, y_pred, zero_division=0)
    auc = roc_auc_score(y_played, y_proba)
    
    # Matrice de confusion
    cm = confusion_matrix(y_played, y_pred)
    tn, fp, fn, tp = cm.ravel()
    
    # === √âTAPE 7: Afficher r√©sultats ===
    logger.info("\n" + "="*70)
    logger.info("R√âSULTATS BACKTEST 2025-26")
    logger.info("="*70)
    logger.info(f"‚úì Matchs √©valu√©s: {len(y_played)}")
    logger.info(f"")
    logger.info(f"üìä M√©triques:")
    logger.info(f"  Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)")
    logger.info(f"  Precision: {precision:.4f}")
    logger.info(f"  Recall:    {recall:.4f}")
    logger.info(f"  F1-Score:  {f1:.4f}")
    logger.info(f"  AUC:       {auc:.4f}")
    logger.info(f"")
    logger.info(f"üìä Matrice de confusion:")
    logger.info(f"  Vrais n√©gatifs (TN): {tn}")
    logger.info(f"  Faux positifs (FP):  {fp}")
    logger.info(f"  Faux n√©gatifs (FN):  {fn}")
    logger.info(f"  Vrais positifs (TP): {tp}")
    
    # === √âTAPE 8: Comparaison ===
    baseline = 0.5479  # Fallback 2025-26
    improvement = accuracy - baseline
    
    logger.info(f"")
    logger.info(f"üìà Comparaison:")
    logger.info(f"  Ancien (fallback):  {baseline:.4f} ({baseline*100:.2f}%)")
    logger.info(f"  Nouveau (V3 live):  {accuracy:.4f} ({accuracy*100:.2f}%)")
    logger.info(f"  Am√©lioration:       {improvement:+.4f} ({improvement*100:+.2f}%)")
    
    # Objectif
    if accuracy >= 0.70:
        logger.info(f"")
        logger.info(f"üéØ OBJECTIF 70% ATTEINT!")
    elif accuracy >= 0.65:
        logger.info(f"")
        logger.info(f"‚ö†Ô∏è Proche de l'objectif (gap: {0.70 - accuracy:.2%})")
    else:
        logger.info(f"")
        logger.info(f"‚ùå Objectif non atteint (gap: {0.70 - accuracy:.2%})")
    
    # === √âTAPE 9: Sauvegarder r√©sultats ===
    results = {
        'timestamp': datetime.now().isoformat(),
        'season': '2025-26',
        'total_matches': len(df),
        'evaluated_matches': len(y_played),
        'metrics': {
            'accuracy': float(accuracy),
            'precision': float(precision),
            'recall': float(recall),
            'f1': float(f1),
            'auc': float(auc)
        },
        'confusion_matrix': {
            'tn': int(tn),
            'fp': int(fp),
            'fn': int(fn),
            'tp': int(tp)
        },
        'comparison': {
            'baseline': float(baseline),
            'new_accuracy': float(accuracy),
            'improvement': float(improvement),
            'improvement_pct': float(improvement * 100),
            'target_reached': accuracy >= 0.70,
            'target_gap': float(0.70 - accuracy)
        }
    }
    
    results_path = Path("reports/backtest_2025-26_results.json")
    results_path.parent.mkdir(parents=True, exist_ok=True)
    with open(results_path, 'w') as f:
        json.dump(results, f, indent=2)
    logger.info(f"")
    logger.info(f"‚úì R√©sultats sauvegard√©s: {results_path}")
    
    # Sauvegarder aussi pr√©dictions d√©taill√©es
    predictions_df = pd.DataFrame({
        'game_id': df.loc[mask_played, 'game_id'],
        'game_date': df.loc[mask_played, 'game_date'],
        'home_team_id': df.loc[mask_played, 'home_team_id'],
        'away_team_id': df.loc[mask_played, 'away_team_id'],
        'actual': y_played.values,
        'predicted': y_pred,
        'proba_home_win': y_proba,
        'confidence': np.maximum(y_proba, 1 - y_proba),
        'is_correct': (y_pred == y_played.values)
    })
    
    predictions_path = Path("reports/backtest_2025-26_predictions.csv")
    predictions_df.to_csv(predictions_path, index=False)
    logger.info(f"‚úì Pr√©dictions sauvegard√©es: {predictions_path}")
    
    logger.info("\n" + "="*70)
    logger.info("BACKTEST TERMIN√â")
    logger.info("="*70)
    
    return results


if __name__ == '__main__':
    results = backtest_2025_26()
    
    print("\n" + "="*70)
    print("R√âSUM√â")
    print("="*70)
    print(f"Accuracy 2025-26: {results['metrics']['accuracy']*100:.2f}%")
    print(f"Am√©lioration: +{results['comparison']['improvement_pct']:.2f}%")
    print(f"Objectif 70%: {'‚úÖ ATTEINT' if results['comparison']['target_reached'] else '‚ö†Ô∏è NON ATTEINT'}")
